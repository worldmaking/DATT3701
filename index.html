<!doctype html>
<html>
<head>
<meta charset="utf-8"/>
<title>DATT3701</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" href="css/skeleton.css">
<style>
/* see https://getskeleton.com/ */
/* html { font-size: 100%; } */
table { width:100%; }
img { width: 100%; max-width: 100%; }
.youtube {
 	max-width: 640px;
    overflow: hidden;
}
.youtube img {
    max-width: 320px;
}
</style>
</head>
<body>
<div class="container">
<div class="row">
<div class="full column">
<h1>FA/DATT 3701 6.00 Collaborative Game Project</h1>
<div id="toc"></div>
</div>
</div>
<div class="row">
<div class="full column" id="main_body">
<script type="bogus" id="sourcetext">


## Course Description

In this year long studio course the class collaborates on the realization of two or three ambitious game projects. Students will work together as a development team by taking on roles where they focus on specific aspects of the project (such as Director, Designer, Artist, Programmer, Level Designer, Sound Designer, Publicity, etc.) The development team structure is modeled on teams used in large-scale project development within fields related to games that rely on multi-stakeholder collaboration and interdisciplinary research. Projects may incorporate partnerships with York-based or external Faculties, Departments, or research teams depending on the focus of the project. The nature of the project themes will vary from year to year, but will be a significant work in the field of games. 

The course instructor(s) will prepare a general description of the project(s) at the beginning of the course. The details of the project(s) will be developed as part of the class activities. As part of the project development and execution students will be expected to prepare presentations, posters, and a written paper. The culmination of this course will be a final presentation in a public venue. In addition to group assignments, students are evaluated based on their individual contribution, teamwork, presentations, and other deliverables as appropriate. 

**Prerequisites:** Only open to students the Digital Media Specialized Honours BA program Games stream. FA/DATT 2300 3.0, FA/DATT 2301 3.0, LE/EECS 2030 3.0 Course credit exclusion: FA/DATT 3700 6.0

**Instructors:** Yifat Shaik & Graham Wakefield

**Time:** Mondays, 1.30pm - 4.30pm

**Location:** ACW 103

Additional resource locations:
- [The Digital Media Art & Technology Learning Lab (ATLL), Accolade West 102](https://dm.ampd.yorku.ca). To use the Digital Media Lab outside of class time, you must purchase a Digital Media Lab Card. Lab Cards are $25 for the year, or $15 for one term. The Digital Media Lab Card can be purchased in the front desk of the Computational Arts/Visual Arts and Art History office, located on the second floor of the Goldfarb Centre for Fine Arts. The office is open Monday to Friday, 8:30am-4:30pm. Once you have paid, take your receipt to Frank Tsonis in ACW 102 and he will issue you a card.
- [Alice lab, GCFA309](https://worldmaking.github.io), with VR and motion tracking systems. 
- AMPD's Motion Media Lab at [Cinespace Film Studios](https://cinespace.com), with an [Organic Motion markerless motion capture](https://www.organicmotion.com). 

**Language of Instruction:** English

## Organization of the Course 

The first part of the course focuses on ideation toward proposal development, including rapid pitching, peer reviews, thematic lectures, tutorials, individual research and experimentation, establishing groups, critique and discussion, collaborative sketching, design document development, project management, alpha development, and internal and external review. We expect to approve two or three projects for production, by groups of between three and six members each. 

The second part of the course focuses on production, culminating in public dissemination, with team members taking on specific roles and responsibilities, supported by periodic demo and test, critique/review, and specialized technical tutorials. The final game must be presented in at least one public event prior to the end of the semester. 

Beside group work, throughout the course you will be asked to maintain a 'devblog' updated weekly, to submit a number of individual written assignments or questionnaires, and contribute generously to in-class discussions. Students must attend all class times, and should expect to spend an average of two to three hours per week on assignments and contributions to group work outside of class time.

Throughout the course, as instructors we will act primarily in two roles. First, we will act as the clients, perhaps akin to publishers or curators, of the projects: we issue the call, we articulate the requirements, and will play the principal role in selecting and refining the proposals and their subsequent production. Second, we will act as pedagogical and technical support: we will lead several guided tutorial sessions in the likely software platforms for projects, followed by focused lab guidance as the projects develop.

## Evaluation

The final grade for the course will be based on the following items weighted as indicated:

**Fall Semester**
- Short written assignments: 20% 
- Group proposals: 20% 
- Group prototypes & presentations: 40% 
- Role contribution: 20%

**Winter Semester**
- Role contribution: 30% 
- Reflective papers: 30% 
- Final project: 40%

**Grading**: The final grade is derived from the mean of the Fall and Winter grades. The grading scheme for the course conforms to the 9-point grading system used in undergraduate programs at York (e.g., A+ = 9, A = 8, B+ - 7, C+ = 5, etc.). Assignments and tests will bear either a letter grade designation or a corresponding number grade (e.g., A+ = 90 to 100, A = 80 to 90, B+ = 75 to 79, etc.)  For a full description of York grading system see the York University Undergraduate Calendar at [https://calendars.registrar.yorku.ca/2010-2011/academic/index.htm](https://calendars.registrar.yorku.ca/2010-2011/academic/index.htm).
**Assignment Submission:** Proper academic performance depends on students doing their work not only well, but on time. Accordingly, assignments for this course must be received on the due date specified for the assignment. ***All assignments must be submitted to complete the course.***

**Lateness Penalty:** Assignments received later than the due date will be penalized five grade points per day that assignment is late. Exceptions to the lateness penalty for valid reasons such as illness, compassionate grounds, etc., may be entertained by the Course Instructor but will require supporting documentation (e.g., a doctor’s letter).

-----

## Schedule

This schedule will be continually updated.

### Fall semester

**Week 1 (Sep 11):**

- Overview of course, procedures, content

- Introductions

- The call for submissions
	- Rationale
	- Examples 
	- Discussion
	
![What is a game?](img/sep11.jpg)

- Task 1: prepare a 4-minute "elevator pitch"
	- An Elevator Pitch is a summary used to quickly and simply define your idea, and its *value proposition.*
	- Your pitch should be concise but compelling
	- Focus on what is **unique** about your idea, how it differentiates itself
	- Contextualize your idea with reference to other work (not necessarily games). 
	- You will only be able to use the classroom computer's web browser to support your pitch.
	- We will maintain the 5 minutes ruthlessly. You must fill all of the time, and no more. 
	- If you had several ideas or variants tell us, but first focus should be on the one you think is most interesting in how it responds to the call
	
**Week 2 (Sep 18):** 

- 4 minute elevator pitches

**Week 3 (Sep 25):** 
 
- Pitches feedback, groupings
- Introduction to VR
 
**Week 4 (Oct 2):** 

Oct 9: University closed

*(Oct 6-8: [IndieCade LA](https://www.indiecade.com/))*

**Week 5 (Oct 16):** 

Unity Tutorial:

<div class="youtube" data-embed="L68qRm23V28"></div>

**Week 6 (Oct 23):** 

Intro to Unreal

<!--Due: Design document-->

**Week 7 (Oct 30):** 

- Game Design Document due
- First level due

Homework: 
- Prepare first demo (working level!) for next week
- Set up your personal dev blog with a post on this week's progress

**Week 8 (Nov 6):** 

- First demos!!!

*Yifat in NY.*

Homework
- Continued development, responding to in-class comments
- Update personal dev blog with this week's progress

**Week 9 (Nov 13):** 

**Week 10 (Nov 20):** 

**Week 11 (Nov 27):** 

Presentation & demo dry-run (internal review)

- Presentation slides up to 15 mins
	- Original concept
		- Motivations
		- Unique/unconventional concept
		- How this is embodied in the design of gameplay, art, etc.
		- Relation to existing games or other media (cite examples)
	- Overview of development so far
		- Screenshots, videos, development files, sketches, interesting code snippets (short!!)
		- Playtest feedback (to the extent it is available)
		- Lessons learned
	- Roadmap for next semester
		- Expectations for final class outcome (be realistic)
		- Examples of where you intend to submit it or go next
- Demo 20 mins
	- There should be a fully working experience of at least one level or key moment of interaction, demonstrating all of the essential components of the gameplay, and some of the art
	- If necessary you may fill in some steps with placeholder elements
- Feedback & discussion 

**Week 12 (Dec 4):** 

Presentation & demo to external reviewers

Dec 11-12th: 14th [MONTREAL INTERNATIONAL GAME SUMMIT](http://www.migs17.com/en/home/)

### Winter semester

**Week 13 (Jan 8):**

Jan 8th: Deadline to submit to the [EXPERIMENTAL GAMEPLAY WORKSHOP](http://www.experimental-gameplay.org) at GDC.

*Graham in California*

**Week 14 (Jan 15):** 

**Week 15 (Jan 22):** 

*(Jan 26-28: [Global Game Jam](https://globalgamejam.org/news/announcing-new-executive-producer-and-committee-2018))*
 
**Week 16 (Jan 29):** 

**Week 17 (Feb 5):** 

**Week 18 (Feb 12):** 

Feb 19: University closed

**Week 19 (Feb 26):** 

**Week 20 (Mar 5):** 

**Week 21 (Mar 12):** 

**Week 22 (Mar 19):** 

*([GDC 2018](https://www.gdconf.com) & [IGF 2018](https://www.igf.com), San Francisco)*

*([SXSW Gaming 2018](https://gaming.sxsw.com/expo/indie-corner/), Austin)*

**Week 23 (Mar 26):** 

Final documentation

**Week 24 (Apr 2):** 

Post-mortem

*(Apr 5: [LevelUp Showcase](levelupshowcase.com), Toronto)*

*(July: [Games for Change](https://www.gamesforchange.org) Festival, NY)*

---

## Projects

### Project God Hands

[GDD](https://docs.google.com/document/d/10PT7x-BKXyykqce7PTDLn4xwV-pU3Q8ET5rJuS-XOFw)

Producer: Alex Zonta

### Nonlinearcat

[GDD](https://drive.google.com/drive/folders/0B2xc3HPupJMtOFZCMVZuZWxyVFE)

Producer: Anny Ky

---

## The call for work

We seek proposals for three new works in the area of computer video games, to be developed to a fully-realized play-tested state suitable for its presentation in a public venue, and dissemination online.

The first requirement: We are specifically seeking works that break central conventions of gaming, or address a paradoxical character of gaming media, in a clearly articulated way. 

The second requiremnt: Each project should further differentiate itself by integrate its theme centrally with at least one non-conventional or newly emerging interface format:

- Alternative controllers (alt.ctrl) including physical computing
- Virtual Reality Head-Mounted Display (probably limited to one team only)
- Augmented / Mixed Reality
- Some other alternative interface format, e.g. audio-driven gaming 

We will introduce and are happy to support mainstream gaming engines including Unreal and Unity, as well as less conventional platforms such as Max/MSP/Jitter, Cinder, JS/WebGL, Processing, Arduino, etc. 

## Rationale for the call

First: The computer/video games industry (AKA interactive entertainment industry) is 45 years old, and stratified. Many problems, crises. For example:

- **781 million** games on Steam
	- 37% of these have never been played
	- Nearly 40% uploaded in the last year
	- Steam abandoned control (Greenlight), replaced with curating program, but already curators of curators...
	- Vast majority are copycats, cheap spinoffs, generic and formulaic stereotypes
	- Investors averse to risky concepts with big budgets; low price points require quick turnaround; both lead to least common denominator
	- Also much easier to generate minor variants of a formula or template than to be genuinely original.

Nobody wants to see a portfolio of mediocrity. Away from the mainstream, there are more experimental corners of independent (and occasionally corporate) games demonstrating many refreshingly unusual approaches. Some of these have become so successful that they have launched entire new genres. Even those that don't attract a lot of attention. The most interesting innovators and movements in art (and elsewhere) are born from rejecting tradition.

Second: the experiences that work well on standard gaming platforms do not always translate well to alternate platforms and interfaces. What makes a new interface or format most interesting may also be how it confounds conventional gaming tropes. Like any new format, it is well worth stepping back from the contemporary world to see bigger pictures. What is the real message of the medium? What can it say that hasn't been said? 

But to stretch expectations or break conventions, we need to identify what they are. So perhaps to begin, ***what exactly is a game?***

<!--
In [Reality is Broken](https://janemcgonigal.com/my-book/) Jane McGonigal identifies  four defining traits within the vast diversity of games (both within & beyond the digital forms). Everything else McGonigal holds are there to enhance these four traits, including immersive graphics, that increase our ability to pay sustained attention:

- **goal(s):** outcomes to achieve, focusing attention and orienting partipation. Purpose.
- **rules:** limitations on how goals can be achieved. "Rules push players to explore previously uncharted possibility spaces. They unleash creativity and foster strategic thinking."
- **feedback system:** Tells players how close they are to achieving goals. Points, levels, scores, progress bars, "the game is over when...", etc. A promise of achievability, and a motivation to continue playing. Variety and intensity of feedback is what differentiates digital and non-digital games. You can feel how attentive the game is to you, and approach the very limits of flow).
- **voluntary participation:** All players knowingly and willingly accept the goal, rules, and feedback. A common ground for participation. Freedom to enter/leave establishes a safe and pleasurable activity.

Bernard Suits sums it up: **"Playing a game is the voluntary attempt to overcome unnecessary obstacles."**, which McGonigal paraphrases: "any well-designed game is an invitation to tackle an unnecessary obstacle". "Freedom to work in the most logical and efficient way is the very opposite of gameplay." For example, golf: choosing to make the work of getting a ball into a hole more difficult. Or Portal: the goal at first is to figure out what the goal is; like many games, some puzzles teach you a new capability (new rules).

Why do unnecessary obstacles make us happy? Hard work that we **choose** to do, with no real danger. The opposite of play isn't work, it's depression: pessimistic sense of inadequacy, despondent lack of activity, being forced into the wrong place. Gameplay gives an optimistic sense of capabilities, invigorating rush of activity, positive valence. Fiero: possibly the most primal emotional rush we can experience. Italian for pride, triumph over adversity. Play activates all neurological/physiological systems of happiness: attention systems, reward center, motivation systems, emotion & memory centers... in a way that passive low-engagement activities (tv, chocolate, window-shopping etc.) don't. Tal Ben-Shahar: **"we're much happier enlivening time rather than killing time."**

## Manifesto for a ludic century (Erik Zimmerman)

Erik Zimmerman's article [on Kotaku](https://kotaku.com/manifesto-the-21st-century-will-be-defined-by-games-1275355204), formed as a manifesto, argues for games as the defining medium of the current century:

> The last century's media was based on information communication: linear, often unidirectional; creating bureaucratic systems. This century's media is non-linear, systemic, modular, participatory, customizable, playful. Environmental: immersive, inhabitable, explorable. Complex systems thinking. 

> It is not enough to merely be a systems-literate person; to understand systems in an analytic sense. We also must learn to be playful in them. A playful system is a human system, a social system rife with contradictions and with possibility.

Ian Bogost's response: [Winning Isn't Everything](https://medium.com/matter/winning-isnt-everything-255b3a26d1cf):

>  "I used to think that games would be the dominant medium of the 21st century. The reality? They're too big, too complex, and too smart for that to be true."

## What is a notgame|nongame|ungame|anti-game?

> "One obvious difference between art and games is that you can win a game. It has rules, points, objectives, and an outcome. Santiago might cite an immersive game without points or rules, but I would say then it ceases to be a game and becomes a representation of a story, a novel, a play, dance, a film. Those are things you cannot win; you can only experience them." [Roger Ebert, "Video games can never be art"](https://www.rogerebert.com/rogers-journal/video-games-can-never-be-art). *(Ebert later conceded that games may indeed be art in a non-traditional sense.)*

Tetris is one of the most famous and addictive games, but it is one that can't be won. Competition and winning are *not* defining features of games. 

[notgames.org](https://notgames.org) - "Can we create a form of digital entertainment that explicitly rejects the structure of games? ... that does not rely on competition, goals, rewards, winning or losing?"

> "a form of entertainment that really doesn't have a winner, or even a real conclusion" - Nintendo president Satoru Iwata ([wikipedia](https://en.wikipedia.org/wiki/Non-game))

Non-games have existed since the early days of video games, although there hasn't been a specific term for them. The most famous large-scale industry examples are SimCity and Second Life, but recently there has been a lot more active creativity outside the normal bounds of gaming, especially coming from the indie community, and more widely considered close to [games-as-art](https://en.wikipedia.org/wiki/Video_games_as_an_art_form). In the last couple of years, some have been called "walking simulators" -- although this doesn't capture the full range of experiences found, the rejection of common tropes and trend toward purposelessness is evident. 

> Now that there's so much willingness, technology and skill to create beautiful virtual worlds, it's disappointing how many videogames cling to obstacle-based designs. As if players have to earn the privilege to explore by passing an irrelevant test. Now is the time to open up this medium, to let go of our affections and addictions, of our loyalty to childhood memories, to open up the beauty of videogames for the world to see... Let design mean the building of bridges and the opening of doors, not the hiding of keys and the cowardly sniper shot. (Michael Samyn, TaleOfTales)

Inevitably, here's another manifesto: [Michael Samyn. Not a Manifesto](https://notgames.org/blog/2010/03/19/not-a-manifesto/)

Related to [infinite games](https://en.wikipedia.org/wiki/Finite_and_Infinite_Games).

A recommended [notgames Tumblr](https://notgames.tumblr.com).

[Notgames festival](https://notgames.colognegamelab.com/exhibition.html), since 2011. The [keynote presentation](https://notgames.org/blog/2011/08/23/notgames-fest-keynote/) for the first.




 It could be different games, non-games, not-games, glitch games, non-standard gameplays, non-standard interfaces, games as art -->

## Reference points:

### Proteus

<div class="youtube" data-embed="gWs_RKXkyu0"></div>

[Proteus](https://www.visitproteus.com) is an "open world exploration game". **It has no specific goals.** Every creature and plant has a unique musical signature. The game world is procedurally generated, creating a unique layout each game. Non-realistic 8-bit style. Slow-paced.

Proteus sold 23,000 copies in first nine months. It achieved 80% rating on metacritic (despite “not being a game”...)

[wikipedia](https://en.wikipedia.org/wiki/Proteus_(video_game))

### Gardenarium

<div class="youtube" data-embed="c92MWml-Gyo"></div>

### Dear Esther

<div class="youtube" data-embed="hlGdbziSwEY"></div>

Dear Esther is an experimental first-person art video game developed by The Chinese Room. The game does not follow traditional video game conventions, as it involves minimal interaction from the player and does not require choices to be made nor tasks to be completed. It instead places focus on its story, which is told through a fragmented, epistolary narrative as the player explores an unnamed island in the Hebrides.

[On Edge: A Chat With Robert Briscoe](https://www.rockpapershotgun.com/2013/10/23/on-edge-a-chat-with-robert-briscoe/#more-173753) - "One of the goals with Dear Esther's art was to make the island look like one big organic sculpture, with rocks, grass and even buildings seemingly growing from its surface... The caves are a unique break in Dear Esther's bleak, open environments that allowed room to explore a more surreal, organic style that was full of metaphors and hidden imagery... One of my pet gripes is that every game today wants to look like it's shot through Michael Bay's sunglasses or JJ Abrams' asshole"

By October 2013 Dear Esther sold over 750,000 copies.

**Everybody's Gone to the Rapture**

<div class="youtube" data-embed="4Gt_6BHhjaM"></div>

Deep within the Shropshire countryside, the village of Yaughton stands empty. Toys lie forgotten in the playground, the wind blows quarantine leaflets around the silent churchyard. Uncover the traces of the vanished community; discover fragments of events and memories to piece together the mystery of the apocalypse. 

### Flow

<div class="youtube" data-embed="9pRBptP3i1Q"></div>

In the fall of 2005, Jenova Chen and Kellee Santiago began thinking about creating their own video game company. The two were in their final year as master's students in the Interactive Media Program at the University of Southern California's School of Cinematic Arts. 

The title is taken from the eponymous psychological mental state. "In positive psychology, flow, also known as the zone, is the mental state of operation in which a person performing an activity is fully immersed in a feeling of energized focus, full involvement, and enjoyment in the process of the activity." - [Wikipedia](https://en.wikipedia.org/wiki/Flow_(psychology)) Mihály Csíkszentmihályi named this and identified it as a state in which the challenges posed by the environment approach a person's optimal capacity to address them. In the game Flow, one is free to descend or ascend levels of difficulty (as literal levels of depth in the water) to achieve an optimal flow state.

Chen went on to found thatgamecompany and produced works such as Flower and Journey.

**Flower**

<div class="youtube" data-embed="Nnxk-USQyfQ"></div>

In Flower, the player controls the wind, blowing a flower petal through the air using the movement of the game controller. Flying close to flowers results in the player's petal being followed by other flower petals. Flower was primarily intended to arouse positive emotions in the player, rather than to be a challenging and "fun" game. The team viewed their efforts as creating a work of art, removing gameplay elements and mechanics that were not provoking the desired response in the players. 

**Journey**

<div class="youtube" data-embed="i_KrjxD8djo"></div>

In Journey, the player controls a robed figure in a vast desert, traveling towards a mountain in the distance. Other players on the same journey can be discovered, and two players can meet and assist each other, but they cannot communicate via speech or text and cannot see each other's names. The robed figure wears a trailing scarf, which when charged by approaching floating pieces of cloth, briefly allows the player to float through the air. The developers sought to evoke in the player a sense of smallness and wonder, and to forge an emotional connection between them and the anonymous players they meet along the way. - [wiki](https://en.wikipedia.org/wiki/Journey_(2012_video_game))

### The Unfinished Swan

<div class="youtube" data-embed="nhyNzFShhIc"></div>

[The Unfinished Swan](https://www.giantsparrow.com/games/swan/) is a game about exploring the unknown. The player is a young boy chasing after a swan who has wandered off into a surreal, unfinished kingdom. The game begins in a completely white space where players can throw paint to splatter their surroundings and reveal the world around them.

Ian Dallas was also a graduate student at the University of Southern California. "He worked on his idea for a game in a class and periodically went to visit a faculty member, Mark Bolas, to show prototypes. One of those days, he showed off “Whitespace,” where **the idea was to explore a 3D world that started out with a blank screen.** That became the core defining game mechanic behind The Unfinished Swan, which Dallas describes as a “first-person painting” game." [How The Unfinished Swan moved from student project to PS3 downloadable game, by Dean Takahashi](https://venturebeat.com/2013/03/25/how-the-unfinished-swan-moved-from-student-project-to-playstation-3-downloadable-game/)

> In the same article, Dallas also says: “There is a difference between knowing the tools and becoming fluent with them... You want to get under the hood and do things they were not meant to do."

The Unfinished Swan won two BAFTA awards in 2013, one for Game Innovation and one for Debut Game.

More perceptually-challenging explorations:

**Antichamber**

<div class="youtube" data-embed="AnJrz2qNMQw"></div>

A nonlinear, non-euclidean, mondrian-esque FPS. 

**Memory of a Broken Dimension**

<div class="youtube" data-embed="6G_IU5lK1E8"></div>

<div class="youtube" data-embed="ZZgbIJOWbB0"></div>

[https://www.brokendimension.com](https://www.brokendimension.com)

**Naissancee**

<div class="youtube" data-embed="uDSi1-lACNk"></div>

Made with Unreal. Featuring soundtrack composed by Pauline Olivieros, renowned since the 1960's. Inspired by Tsutomu Nihei's famous [Blame!](https://en.wikipedia.org/wiki/Blame!) manga of endless corridors. 

(Another Blame-inspired game that looks interesting [in development here](https://bac9.tumblr.com). Here's [his project blueprint](https://imgur.com/gwdkMgg).)

**Scanner**

<div class="youtube" data-embed="W0uhjXwEQY0"></div>

[Nice feature giving detail on the meshing of technique and story on KillScreen](https://killscreen.com/articles/upcoming-videogame-explore-world-visual-scanner/).

**Devil's Tuning Fork**

<div class="youtube" data-embed="_tKF_subEMA"></div>

A game created by the [DePaul Game Elites team at DePaul University's College of Computing & Digital Media in Chicago.](https://devilstuningfork.com)

### The Stanley Parable

<div class="youtube" data-embed="gblvOhnv2k0"></div>

An interactive fiction video game designed by Davey Wreden. Originally a mod of Half Life 2, it was later rewritten with the Source engine and launched through Steam Greenlight. There are no combat or other action-based sequences. Instead, the player guides Stanley, the game's protagonist, through a surreal environment while the narrator, voiced by British actor Kevan Brighting, delivers exposition. The player has the opportunity to make numerous decisions on which paths to take, and because at times the narrator says what Stanley will do next, the player can choose to ignore the narration and make a different choice. Every choice made by the player is commented on by the narrator, and depending on the choices the player makes, they will encounter different endings to the game before it restarts.

**The Beginner's Guide**

<div class="youtube" data-embed="OPP9pdApRQE"></div>

Narrated by Davey Wreden, it takes the user through a number of incomplete and abstract game creations made by a developer named Coda. Wreden challenges the player to try to come to understand the type of person Coda is from exploring these spaces in a first-person perspective. Within the narrative, the player discovers that Wreden had tried to force meaning onto Coda's games, causing them to end their relationship. 

### Bientot l'&eacute;t&eacute;

<div class="youtube" data-embed="QJe5O7b5JYo"></div>

[From Tale of Tales](https://tale-of-tales.com/bientotlete/)

> Forbes - “Of course, basing a computer game set in the distant future on an examination of the quiet desperation of bourgeois French womanhood in the 1950s is, itself, going to look like non-linear art game weirdness. But there is a plan.” 

### Noby Noby Boy

<div class="youtube" data-embed="d7tisMyOMhk"></div>

[Noby noby boy](https://en.wikipedia.org/wiki/Noby_Noby_Boy)

### Orchids to Dusk

<div class="youtube" data-embed="CWqamSu3qFA"></div>

[Orchids To Dusk](https://polclarissou.itch.io/orchids-to-dusk)

> Orchids to Dusk is a short contemplative wandering experience by Pol Clarissou with music by Marskye. You are an astronaut stranded on an alien planet, with only a few minutes left to live. 

### Niva

<div class="youtube" data-embed="Ytc2BqBhHXg"></div>

[Niva](https://slyce.itch.io/niva)

> NIVA is a pacifistic exploration art game. The player slips into the role of a mighty forest god to restore the harmony in a mesmerizing forest and relieve it of a mysterious infestation. This shuddersome, mothlike infestation is drawn to the conflicts of the forest's inhabitants. Through observation and by using the abilities of nurturing and withering fascinating plants the player can solve said conflicts. NIVA's scenic art style, relaxing music and simple but intriguing game mechanics invite to explore the forest and have a rest from the stressful everyday life.

### Walden

<div class="youtube" data-embed="OEJ_59hVPgw"></div>

[Walden](https://gameinnovationlab.itch.io/walden)

> Play as philosopher and naturalist Henry David Thoreau in his experiment in self-reliant living at Walden Pond. Live off the land, seek out the small wonders and beauties of the woods, and find balance between your need to survive and your desire to find inspiration.


### Localhost

<div class="youtube" data-embed="tg1D-pVhaxk"></div>

[Localhost](https://sophiapark.itch.io/localhost)

> It's your first day on the job in the last days before the singularity. You're undoubtedly a valuable addition to our team. We hope you'll find in our era of machine governance, the repair trade is a noble pursuit.

### Mu Cartographer

<div class="youtube" data-embed="lAiUeZUmtuc"></div>

[Mu Cartographer](https://titouanmillet.itch.io/mu-cartographer)

> Mu Cartographer is a contemplative game experience that combines colourful sandbox and experimental treasure hunt.


### Everything

<div class="youtube" data-embed="JYHp8LwBUzo"></div>

[Everything](http://www.davidoreilly.com)

> Everything is a beautiful new interactive experience from David OReilly, narrated by the late great philosopher Alan Watts.

### Ian Cheng

<div class="youtube" data-embed="ixydT753_Ag"></div>

[Ian Cheng](http://iancheng.com) -- [wikipedia page](https://en.wikipedia.org/wiki/Ian_Cheng)

One of the few artists to have exhibited game/simulations at major art events and venues (such as MOMA New York). Cheng creates 'simulations' as games that play themselves.

### Harun Farocki

<div class="youtube" data-embed="Yzc3OPc8gUM"></div>

[Ian Cheng](http://www.harunfarocki.de) -- [wikipedia page](https://en.wikipedia.org/wiki/Ian_Cheng)

Harun Farocki (9 January 1944 – 30 July 2014) was a German filmmaker, author, and lecturer in film. Some of his last works were documentary studies into the creation and use of videogame technology. The Parallel I-IV series, his last work, "he scans the history of computer games, beginning with blocky linear animation, mainly of landscapes, from the 1980s. As the sophistication increases, so does the violence quotient, until it seems that everything is framed in the all-seeing sights of a high-powered gun: war, universal or at street level, is a big-boy game." [Harun Farocki: Parallel. Holland Cotter. New York Times, Sep 18 2014.](https://www.nytimes.com/2014/09/19/arts/design/harun-farocki-parallel.html?mcubz=1)

---

More!

[Edible Games](http://jennsand.com/edibleGames/overview.php)

[Robert Yang](http://www.debacle.us/#work)


----

## Proposals

General Questions

- Can you identify a core mechanic? Can you identify some secondary mechanics?
- Can you distill your concept into on line (a tag line)? And if you can, does it still sounds good?
- How will the player interact with the game? Think about the verbs of the interaction
- Where will the game be played? How will it influence the digital space?
- Does the technology you choose complement the mechanics? Is it necessary to use it? Can you use an alternative? 
- What is the theme of your game? Does the mechanics and technology compliment the theme?
- What emotion/feeling do you want to invoke? How are you planning to make the player feel those emotions
- Games are based on rules. These might be embedded into systems, mechanics, physical simulations, etc., but they are still rules. Your game will have to have its core rules. What makes a really good rule? What makes it interesting? Why?

The godhands one (Nicholas)

- Core mechanic is really very interesting, but needs to be pushed more (think of scale). Map out some specific ‘moments’ in the experience, it’s plot beats, the singular moments that would be what you remember from playing it, etc. Map out some of the possible interplays between the two modes. 
- Be careful not to optimize the ideas too early, some details seemed to be distractions from exploring the possibilities of the mechanics in broader strokes.  Give enough space for this game to mutate into many shapes before committing to anything.
- Consider different controllers for different players, consider alt controllers too. Choose what fits to or further liberates the core potential of the game. 
- Watch this: https://vimeo.com/33931449 (“Sandbox” at Santa Monica Beach (2010) by Rafael Lozano-Hemmer)
- Watch this: https://www.youtube.com/watch?v=WAA9uYxgSbg (Videoplace (1985), by Myron Krueger)

The bad guy lair one (Andrew)

- The humour is great, as is the LARPiness. Humour and cuteness however usually isn’t enough sustain interest — once you heard a joke it usually gets less funny. The humour here can become part of the unique logic and ambience of the experience, but that’s not usually enough to drive it forward.
- Related: What binds the mini-games together in a way that this experience is not just a + b + c but actually becomes something else, it’s own unified whole?
- There’s a way this plays well into the affordances & constraints of VR (the fixed location, the mini-game physics play of motion controllers a la Job Simulator), but this idea could work well without VR too. 
- is it a VR game? 

The fractured glasses one (Nate)

- As presented, this wants to be a single-player game, not two-player.
- The mechanic of the sound/vision fracture works well for VR and does present a basis for some very interesting possible twists that go beyond what you have suggested so far
- However, the ‘need to fix things’ is a well-used narrative in games. The pairing of VR with horror (and sensory deprivation) is also widespread already. You need to assume these as a baseline to improve upon and more interestingly differentiate from.
- For example, how does horror really work? How does the ‘fix things and see better’ narrative sustain interest? E.g. the deepest horror is when what you thought was really a hallucination is actually real -- the descent into madness. What if you find out your glasses are not telling you the truth you thought they were? What if there’s a way in which the monster is not visible at all? What if there’s a way the monster is actually you? 

The dream eater one (Ann)

- The subject is very interesting, and has lots of potential. It definitely needs to be pushed more. 
- There’s something that overlaps with contemporary disturbing concerns with the amount we rely on our media to be reliable… (#fakenews and all that)
- There’s certainly a truth that our memories are creative: each time you remember something, it gets modified. And we perceive a lot less of the world than our brains make us think. Etc. 
- Please try to flesh out: What it means that it is a ‘spatial narrative’? Why sound and image ambience is so important? How do fake dream fragments affect the future? Is there a collective unconscious/noosphere element to the dreams? Etc.
- There’s a possible project here in combination with the broken glasses one — a sympathetic resonance with the fixing of visions and supernatural/hallucinations. It might be that trying to reconcile horror and dream/folk tale could give birth to something even richer.
- As it stands it doesn’t *need* to be in VR. So if it is VR the mechanics really need to have a reason to exist as is. The entering the dream moment could be part of it?

The different worlds of player and spectator one (Anny)

- The player vs. spectator aspect aroused the most interest here. That in some way we live in the same world, in other ways the worlds are different, and this changes the meaning of our actions in potentially shocking ways. 
- The current problem is the surprise factor — once the surprise is revealed, the game is really over. And it’s over as soon as you hear about it, before you even play it. The cute/monster contrast is too simple. More subtlety in the mechanic is required here. So: is there a way to rethink this game in a way that the player can’t spoil the surprise? That, whatever the player says about what they experienced, won’t break the spell for spectators when they become players next?
- It will need to be in 3D. The VR part here seems essential as this is the only way to ensure that the player can’t see what the spectators see. 
- The dual screen can be solved one way or another. 

The nonlinear time one (June)

- We loved the opening gambit: “a game with no structure”. Of course the game probably does have structure, quite a bit of it, but it might not feel like that to the player. Again, loved the comment that linearity is too much of a guide. It tells you where to go next etc. So take away the guide.
- The mapping of movement to time passing (so standing still stops time) is famously used in Braid. You should certainly look at all the variations of time-movement mapping in that game and consider that a baseline to differentiate against. There are some games that slow time or mess around with space in other ways. If you made time stop when you are moving (i.e. inverse relation), then it would seem to everyone else in the world that you are teleporting. Overall it’s not clear whether this needs to be part of the game. 
- Does time-travel bifurcate history? I.e., if you travel back & change the past, does the future change too, or is it always somehow predestined to work out the same? If you can’t change the past then the game is really all about writing an amazing script, but the alternative nonlinear time aroused more interest. It will need some procedural solutions. The idea of bringing objects through the time jumps opens up a realm of puzzle-like strategies for example -- but what if these objects are autonomous too (animals, robots, etc.)?  What happens if you jump back to where you were before -- is the old you still there?
- Overall there appear to be several possible games here. The space of possible games at the moment is too large, and needs to be narrowed down significantly (without losing depth!)  Which is the most interesting, and how can it be as powerful as possible? Focus on the core inspirations (game with no structure, take away the guide of time by nonlinearity) and keep trying variants until you find the best. 


----

## Introduction to VR

- "An artificial world that consists of images and sounds created by a computer and affected by the actions of a person who is experiencing it." - Merriam-Webster
- "Computer technologies that use software to generate realistic images, sounds and other sensations that replicate a real environment (or create an imaginary setting), and simulate a user's physical presence in this environment, by enabling the user to interact with this space and any objects depicted therein using specialized display screens or projectors and other devices. " - Wikipedia

VR has also been defined as inducing targeted behaviour (such as having a designed experience) in an organism by using artificial sensory stimulation, while the organism has little or no awareness of the interference. The 'no awareness' condition has elsewhere been more eloquently articulated as **'the illusion of non-mediation'**. 


### A brief history of VR

![cave](http://upload.wikimedia.org/wikipedia/commons/1/1e/Lascaux_painting.jpg)

30,000 BCE-: From the firelit cave paintings of Lascaux to the birth of painting, architecture, and other arts, we have been attempting to recreate both the world around us and our imagination within.

![butterfly](http://upload.wikimedia.org/wikipedia/commons/c/c1/Dschuang-Dsi-Schmetterlingstraum-Zhuangzi-Butterfly-Dream.jpg)

4thC BCE: Zhuangzi dreams he is a butterfly, but questions if he is a butterfly dreaming he is a man. Are dreams also simulations? 

> Once Zhuang Zhou dreamed he was a butterfly, a fluttering butterfly. What fun he had, doing as he pleased! He did not know he was Zhou. Suddenly he woke up and found himself to be Zhou. He did not know whether Zhou had dreamed he was a butterfly or a butterfly had dreamed he was Zhou. Between Zhou and the butterfly there must be some distinction. This is what is meant by the transformation of things.
> During our dreams we do not know we are dreaming. We may even dream of interpreting a dream. Only on waking do we know it was a dream. Only after the great awakening will we realize that this is the great dream.

![cave](http://upload.wikimedia.org/wikipedia/commons/b/b1/Platon_Cave_Sanraedam_1604.jpg)

~380 BCE: Plato likens the uneducated to prisoners in a cave unable to turn their heads. A fire behind them casts shadows of puppets, also behind them, such that all they can see are the puppets' shadows on the wall in front. Such prisoners mistake appearance for reality. (The allegory is intended to show that the names we give for things, to allow us as prisoners to converse about what we see, are in fact names for things that we cannot see, but only grasp with the mind. That is, the real meaning of the words we use is not something that we can ever see with our senses alone. But we can only know this by being liberated from the illusion of the shadows.)

1637-1672: René Descartes invents conventions for analytic geometry and algebraic approaches to geometry; for which reason we still describe space in X, Y and Z axes and call this "Cartesian" coordinates. He believed that algebra was a method to automate reasoning. 

Descartes also uses methodological skepticism to question his existence and perception, and whether he is dreaming or things are externally real. Influenced by the mechanical automatons of his time, he draws attention to the problem of the connection between body and mind, inadvertently launching a dualism that dominates Western thought thenceforth and remains an influence over and problem of VR. 

> "VR opens the door to what Jaron Lanier (who coined the term virtual reality in the 1980s) calls “post-symbolic communication”: No longer are we limited to communicating via sequences of symbols represented by audible vibrations of our vocal chords, or produced by our fingers pressing on a series of keys or, more recently, a flat piece of glass. Instead, you experience my dream directly, without having to interpret long strings of verbal or written symbols... The medium, the place where those stories will unfold, exists within our consciousness. We’ll find ourselves having passed through our long-held, precious frames to live within those stories. And we’ll carry the memory of those stories not as content that we once consumed, but as times and spaces we existed within." - [source](http://virtualrealitypop.com/futureofvr-8be30f0fca6a#.n1s3d4n92)

![panorama](http://upload.wikimedia.org/wikipedia/commons/3/3a/Cross-section-of-the-rotund_0.jpg)

1800's: The popular wave of massive-scale panorama paintings, often with dedicated buildings, usually depicting landscapes and/or historic events. 

At the same time, the first attempts to capture permanent images from camera obscura (themselves inspired by caves...) through chemical means marks the birth of photography.

1838: Sir Charles Wheatstone invents [stereoscopic photography](http://www.youtube.com/watch?v=Pu6SOckMxT0&feature=youtu.be).

1885/1935: L'Arrivée d'un Train 

<div class="youtube" data-embed="b9MoAQJFn_8"></div>

The train moving directly towards the camera, shot in 1895, was said to have terrified spectators at the first screening, a claim that has been called an urban legend. What most film histories leave out is that the Lumière Brothers were trying to achieve a 3D image even prior to this first-ever public exhibition of motion pictures, and later re-shot the film in stereoscopic 3D, first screened in 1935. Given the contradictory accounts that plague early cinema and pre-cinema accounts, it's plausible that early cinema historians conflated the audience reactions of the 2D and 3D screenings of L'Arrivée d'un Train.

1901: L. Frank Baum, an author, first mentions the idea of an electronic display/spectacles that overlays data onto real life (in this case 'people'), it is named a 'character marker'.

1935: Stanley G. Weinbaum's short story "Pygmalion's Spectacles" describes a goggle-based virtual reality system with holographic recording of fictional experiences, including smell and touch: "You are in the story, you speak to the shadows (characters) and they reply, and instead of being on a screen, the story is all about you, and you are in it."

![viewmaster](http://upload.wikimedia.org/wikipedia/commons/f/f5/View-Master_with_Reel.jpg)

1939: The ViewMaster stereoscopic device is launched.

1943: Patent filed for a head-mounted stereo TV.

1929-1950s: Link Trainer, a mechanical flight simulator with motion simulation, used by over 500,000 pilots.

![3dcinema](http://i2.cdn.turner.com/money/dam/assets/160726073325-3d-glasses-1952-780x439.jpg)

1950s-60s: The "golden era" of 3D cinema. 

![sensorama](http://upload.wikimedia.org/wikipedia/commons/d/dc/Sensorama-morton-heilig-virtual-reality-headset.jpg)

1957–62: Morton Heilig, a cinematographer, creates and patents a mechanical simulator called Sensorama with visuals, sound, vibration, and smell. Heilig later (1960) filed a patent for a multisensory HMD.

![hmd](http://patentimages.storage.googleapis.com/pages/US2955156-1.png)

> "When anything new comes along, everyone, like a child discovering the world, thinks that they've invented it, but you scratch a little and you find a caveman scratching on a wall is creating virtual reality in a sense." - Morton Heilig

![philco](http://wearcam.org/ar/philco_hmd_l.png)

1961: Philco Headsight is the first HMD, used for remote camera viewing (CCTV), including head orientation tracking.

1963: Ivan Sutherland's Sketchpad, one of the first interactive graphics program.

<div class="youtube" data-embed="USyoT_Ha_bA"></div>

[![Personal Television from Life Magazine.](http://cdn.arstechnica.net/wp-content/uploads/2010/04/tvglasses.jpg)](http://arstechnica.com/tech-policy/2010/05/ralph-124c-41-a-century-later/)   
Hugo Gernsback (of "Hugo Awards" fame), wearing his TV Glasses in a 1963 Life magazine shoot.

1964: New York inventor and holographer Gene Dolgoff, who is also the inventor of the digital projector, creates a holography laboratory. Dolgoff's obsession with holography included theories of "matter holograms", the holographic nature of the universe, and the holographic nature of the human brain. 

1965: Ivan Sutherland pens [The Ultimate Display. Ivan E Sutherland, 1965](http://worrydream.com/refs/Sutherland%20-%20The%20Ultimate%20Display.pdf), inspiring everything from the Holodeck to the Matrix. 

> "The ultimate display would, of course, be a room within which the computer can control the existence of matter. A chair displayed in such a room would be good enough to sit in. Handcuffs displayed in such a room would be confining, and a bullet displayed in such a room would be fatal. With appropriate programming such a display could literally be the Wonderland into which Alice walked."

![Sword of Damocles](http://blog.modernmechanix.com/mags/qf/c/PopularScience/4-1971/med_vr_goggles.jpg)

1968: Ivan Sutherland's Sword of Damocles, widely considered to be the first virtual reality (VR) and augmented reality (AR) head-mounted display (HMD) system. DARPA. 


<div class="youtube" data-embed="NtwZXGprxag"></div>

The next twenty years see slow but non-stop development of VR technologies largely within military, industry, and science research institutions, with a slow infiltration into popular culture.

![holodeck](img/holodeck.jpg)

1974: The Holodeck concept appears in Star Trek: the Animated Series, and reappears in 1987 in Star Trek: The Next Generation.

1975: Myron Krueger creates Videoplace to allow users to interact with virtual objects for the first time. Book "Artificial Reality" articulates an artform whose primary material is real-time interaction itself.

<div class="youtube" data-embed="dmmxVA5xhuo"></div>

1977: Star Wars features a hologram (Leia's message for Kenobi) and some of the first widely-seen 3D computer graphics in film (the Death Star plans).

![aspen](http://upload.wikimedia.org/wikipedia/commons/4/48/QADAS.jpg)

1978: [Aspen Movie Map](http://en.wikipedia.org/wiki/Aspen_Movie_Map) -- a proto Streetview, interactive via laserdisc, that also had a polygonal mode.

<div class="youtube" data-embed="2Ytd12d6qNw"></div>

1979: LEEP HMD with lenses designed for very wide field of view.

1980: Steve Mann creates the first wearable computer, a computer vision system with text and graphical overlays on a photographically mediated reality.

![battlezone](http://cdn.mos.cms.futurecdn.net/e3a677f2f9d2dc35ec1a16862d376c25-650-80.jpg)

Battlezone is the first big 3D vector graphics success in arcade games. Battlezone was thought so realistic that the US Army used it to train tank gunners.

1982: Atari founds a VR research lab

[Tron](http://en.wikipedia.org/wiki/Tron) movie

1983: Brainstorm movie.

1984: William Gibson writes [Neuromancer](http://en.wikipedia.org/wiki/Neuromancer), bringing wide acclaim to the cyberpunk genre.

![elite](https://upload.wikimedia.org/wikipedia/en/c/c4/BBC_Micro_Elite_screenshot.png)

Elite, an open world space trading video game, published by Acornsoft for the BBC Micro and Acorn Electron computers, featuring revolutionary 3D graphics

1985: Jaron Lanier (formerly of the Atari lab) coins the phrase Virtual Reality and creates the first commercial business ("VPL") around virtual worlds.

VR at NASA:

<div class="youtube" data-embed="NAuytnYU6JQ"></div>


[1988: The Legible City. Jeffrey Shaw](http://www.jeffrey-shaw.net/html_main/show_work.php?record_id=83)

<div class="youtube" data-embed="61l7Y4MS4aU"></div>

- Drift / derive in a city of words
- Why words? The synthetic image is a language -- it requires algorithm (code), and it is a language town. 
- It brings together two worlds of physical and virtual. The mechanical phase of the body: "I am the one that activates the machine," with the machine as imaginary language. 
- But the print run is false, because the city is already given in advance. The idea of freedom is also false, because everything is already pre-calculated, planned ... and foreshadowed. We are increasingly a prisoner of language.

1989: [Shadowrun](http://en.wikipedia.org/wiki/Shadowrun) desktop role-playing game in a near-future cyberpunk + VR world

The 90's saw a wave of public interest and hype in VR, which as it grew became often conflated with cybernetics, AI, computer graphics in general, the nascent internet, etc. as *cyberspace*.

1991: Virtuality company launches with a new multiplayer hardware prototype in several countries -- but at $73,000 per unit! Sega also launches a VR headset for their console.

![CAVE](http://upload.wikimedia.org/wikipedia/commons/6/6d/CAVE_Crayoland.jpg)

EVL in Chicago launches the first cubic CAVE VR system. Later commercialized by Mechdyne, WorldViz and others, still actively installing new systems in research labs around the world today.

Retinal display developed, scanning images onto retina, commercialized by Microvision. (Antecedent of tomorrow's MagicLeap).

Computer Gaming World magazine predicted "Affordable VR by 1994"

ABC Primetime covers the VR scene (from [vrtifacts.com](http://vrtifacts.com/virtual-reality-1991-many-believe-it-will-revolutionize-the-way-we-live/)):

<div class="youtube" data-embed="c5ZnWNilMxw"></div>

1992: Neal Stephenson writes [Snow Crash](http://en.wikipedia.org/wiki/Snow_Crash)

Lawnmower Man movie.

Sega Virtua Racing, and Virtua Fighter (1993) popularized polygonal 3D games.

1994: The first version of Virtual Reality Modeling Language (VRML), a standard for sharing interactive  3D vector graphics on the web, and by 1997 several 3D chat environments exist.



[1994: Topological Slide. Michael Scroggins & Stewart Dickson.](https://michaelscroggins.wordpress.com/topological-slide/)

<iframe src="https://player.vimeo.com/video/137575437" width="720" height="540" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

[Paper: Absolute Animation and Immersive VR](https://michaelscroggins.files.wordpress.com/2013/02/absolute_animation_and_immersive_vr.pdf).

> "The 'rider' will wear a head mounted display enabling an interactive wide-angle stereo view of a three-dimensional space.  The space will consist of a model of a topological surface to which the platform is bound and upon which it is free to slide.  The ‘rider’ may traverse the model’s surface by leaning in the direction in which she desires to move.  The amount of lean in a given direction will determine the rate of sliding."

1995: Maurice Benayoun creates a VR artwork **Tunnel under the Atlantic** connecting the Pompidou Centre in Paris and the Museum of Contemporary Art in Montreal with 3D modeling, video chat, spatialized sound, and AI.

Strange Days and Johnny Mnemonic movies.

[1995: Osmose. Char Davies](http://www.immersence.com/publications/char/2004-CD-Space.html)

<div class="youtube" data-embed="54O4VP3tCoY"></div>

- Char Davies was painter, became co-founder of SoftImage (-> Autodesk)
- She wanted to demonstrate medium's potential, and "aspects related to the medium of "virtual reality" that are often overlooked"
- Subvert conventional approaches that reinforce an outdated dualist (and masculine) worldview. 
- She redefines immersive virtual space as a medium for de-habituating perception and re-sensitizing us to our own being in the world.
- "Evoke rather than illustrate"; metaphors, aviod solid objects, use translucencies
- Explicit parallel with deep-sea diving (floating, breathing to rise/fall, leaning to move)

![@9.23: scene map](img/osmose_scene_map.png)

[A mini documentary](http://youtu.be/bsT59fp8LpY)

![Osmose](http://www.immersence.com/centralizedImages/osmose/Osm_Tree_600@2x.jpg)

---

However, the same year was also identified as the 'death of VR'. Nintendo releases [VirtualBoy](http://en.wikipedia.org/wiki/Virtual_Boy) for US$ 180, and discontinues it just six months later. [("Nail in the coffin for 90's VR")](http://vrtifacts.com/virtual-boy-another-perspective/) A survey by Computerworld magazine in 2007 listed VR as the 7th biggest technology flop in history.

**What went wrong?**

- Inadequate Image Resolution
- "Motion to photon latency" too high
- Limited Position Tracking
- Cumbersome Equipment
- Lack of Interpretation of Body Movements
- Simulation Sickness
- Cost
- Slow computers
- Poor software design
- Lack of data/understanding the human body, lack of haptics research etc.
- Premature launches & inflated expectations
- Charlatans
- Concern about liability (user accidents)
- Single-user problem
- No consumer "killer app"

 
![quake](http://cdn.mos.cms.futurecdn.net/95c21aa0e5964acbd5ace2d37740fa6a-650-80.jpg)

1996: Quake pioneers play over the Internet first-person shooters. 

3dfx Interactive released the Voodoo chipset, leading to the first affordable 3D accelerator cards for personal computers. [Within a few years dedicated 3D graphics processing unit cards (GPUs) become essential for most video games, and GPU performance wars rapidly increase real-time 3D rendering capabilities at consumer price levels.](http://www.techradar.com/news/gaming/the-evolution-of-3d-games-700995/2)

[1997: World Skin. Maurice Benayoun](http://www.lepixelblanc.co/world-skin). 

<div class="youtube" data-embed="I6NRSD7fBTw"></div>

> Interactive digital artwork. Materials, techniques and processes: Cave automatic virtual environment (CAVE), webcam, camera, printer.

---

Meanwhile, although VR was still capturing some SF attention and slowly being rediscovered through the web, VR develops mainly in research labs, and steadily continues to grow in big-budget industrial, science & health research, as well as military training, outside the media radar.

> "VR was used to visualize oil fields and to visualize machinery to extract oil more efficiently from old fields. Similar things happened in medicine. We understand more about large molecules, we understand more about how the body heals from surgery through VR simulations." - [Whatever happened to VR -- interview with Jaron Lainer (2007)](http://www.10zenmonkeys.com/2007/03/09/whatever-happened-to-virtual-reality/)

1999: [The Matrix](http://en.wikipedia.org/wiki/The_Matrix) movie, and eXistenZ.

2001: Grand Theft Auto III released, popularizing open world games with a non-linear style of gameplay

2005: [The AlloSphere](http://www.allosphere.ucsb.edu)

<div class="youtube" data-embed="u-D-zEToJQ4"></div>

Over this period it also gradually begins to appear on the web.

![SL](http://upload.wikimedia.org/wikipedia/commons/c/c6/Second_Life_11th_Birthday_Live_Drax_Files_Radio_Hour.jpg)

1999: Entrepreneur Philip Rosedale forms Linden Lab to develop hardware for 360 degree VR, but this soon transforms into a platform for 3D socializing, launching SecondLife in 2003.

2007: Google Streetview launched.

**VR goes into the garage, then goes mainstream again**

2009: [A teenage Palmer Luckey announces on a BBS post his home-made Oculus "Rift" HMD.](http://www.mtbs3d.com/phpbb/viewtopic.php?f=120&t=14777)

2011: Now 18, Palmer hacks together a rough prototype in his parents’ garage in Long Beach, California.

2012: John Carmack (lead programmer of Doom, Quake, and many other pioneering 3D games) introduces a duct taped head-mounted display based on Luckey's prototype at the Electronic Entertainment Expo. Palmer's company, Oculus VR, launches [a Kickstarter campaign](http://www.kickstarter.com/projects/1523379957/oculus-rift-step-into-the-game) to fund the development of the Rift. It is phenomenally successful,  raising US$2.4 million for the development of the Rift. 

<div class="youtube" data-embed="DhcOMOWRMnA"></div>

2013: First Oculus Rift developer kit (DK1) ships, for $300. Developer kits are released to give developers a chance to develop content in time for the Rift's release; these have also been purchased by many virtual reality enthusiasts for general usage.

2013: Google announces an open beta test of its Google Glass augmented reality glasses.

[Birdly](http://birdly.zhdk.ch/about/)

<div class="youtube" data-embed="JApQBIsCK6c"></div>

[The Machine to be Another](http://www.themachinetobeanother.org)

<div class="youtube" data-embed="i51Xd9VzzxY"></div>

> Gender Swap is an experiment that uses themachinetobeanother.org/ system as a platform for embodiment experience (a neuroscience technique in which users can feel themselves like if they were in a different body). In order to create the brain ilusion we use the immersive Head Mounted Display Oculus Rift, and first-person cameras. To create this perception, both users have to synchronize their movements. If one does not correspond to the movement of the other, the embodiment experience does not work. It means that both users have to constantly agree on every movement they make. Through out this experiment, we aim to investigate issues like Gender Identity, Queer Theory, feminist technoscience, Intimacy and Mutual Respect. 

2014: Second Oculus Rift developer kit (DK2) ships, for $350. More than 100,000 DK2's shipped by 2015. Oculus VR is acquired by Facebook for $2 billion.

[SightLine: The Chair](http://sightlinevr.com), by Frooxius

<div class="youtube" data-embed="SUH7gWS96Hs"></div>

> "This experience is based off the gaze-direction mechanics of the award winning prototype "SightLine", originally developed for the 2013 VR Jam sponsored by Oculus VR and IndieCade."

2015: Microsoft announces HoloLens augmented reality headset.

HTC partners with Valve Corporation to develop the HTC Vive headset and controllers, released early 2016.

[Tiltbrush](http://www.tiltbrush.com)

<div class="youtube" data-embed="uFWw6hGIKmc"></div>

---

2016: "the year of VR"

- Sony, Facebook, Google, Microsoft, Samsung, Valve, nVidia, Apple and many other large corporations gambling on VR's success. 
- Why now?
	- Technological feasibility & affordability
		- Advances in small displays (thanks to cellphone industry)
		- Advances in 3D graphics (thanks to gaming industry)
	- Gaming industry crisis? Looking for the next big thing?
- Slow rise/uptake. Still not exactly cheap!
- 360 video vs. "real" VR (becoming a polemic)
- Seated vs. room-scale VR (perhaps more temporary)
- SteamVR vs. Oculus Home vs. Viveport (content delivery networks)

Hardware:
- Two consumer PC-tethered headsets (Oculus Rift, HTC Vive) 
	- started shipping in May, hit major stores by summer.
	- currently tethered to PC by cable, but untethered expected to come
- One console-based headset (Playstation VR)
- Several cellphone-based devices 
	- Better quality: GearVR
	- More budget: Google Cardboard, Daydream
	- lower resolution/FPS, but $99 or less + phone
	- position tracking developing
- Lots of other variants and upcoming devices, e.g. OSVR
	
**Rift | Vive + PC specs**

- **Resolution**: 2160 x 1200
- **Refresh rate**: 90 fps
- **Field of view**: 110 degrees
- **Tracking area**: 5 x 11 feet | 15 x 15 feet
- **Video**: no input | front-facing camera
- **Audio**: mic + headphones | mic + headphone jack
- **Price**: ~USD$800 including hand controllers

But that price doesn't include the PC... add another $2000 or so for:

- **GPU:** NVIDIA GTX 970 / AMD R9 290 equivalent or greater
- **CPU:** Intel i5-4590 / AMD FX 8350 equivalent or greater
- **Memory:** 8GB+ RAM
- **Video Output:** HDMI 1.4 or DisplayPort 1.2 or newer
- **USB:** 3x USB 3.0 ports plus 1x USB 2.0 port
- **OS:** Windows 7 SP1 64 bit or newer

2017 onward:

Untethered HMDs and improved display, hand-tracking, eye-tracking, etc.

[![vrfund](http://www.thevrfund.com/wp-content/uploads/vr_industry_aug2016.png)](http://www.thevrfund.com)

It's not 1995 -- there's huge investment, affordable platforms, ready authoring tools and delivery networks, and proven use cases. 

[VR and AR are the next mega tech themes through to 2030; with today likened to the state of mobile phones 15 years ago. Major mainstream adoption predicted for 2025. VR will be the first phase, followed by AR. Content market expected to reach $5.4B by 2025.](http://cdn.instantmagazine.com/upload/4666/piperjaffray.f032beb9cb15.pdf)

Soon-to-emerge tech in AR: Microsoft HoloLens, Meta, Magic Leap (massive investments), Tango, etc.

[Augmented/Virtual Reality revenue forecast revised to hit $120 billion by 2020](http://www.digi-capital.com/news/2016/01/augmentedvirtual-reality-revenue-forecast-revised-to-hit-120-billion-by-2020/#.WBCt8Fdy7y8)

![hype cycle](http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Gartner_Hype_Cycle.svg/559px-Gartner_Hype_Cycle.svg.png)

- Are we in the beginning of the plateau of productivity, or in another hype cycle? [For more on hype cycles](http://www.gartner.com/newsroom/id/2575515) -- Gartner in 2013 placed the plateau for VR in the 5-10 year range.

> (See also [The Rise and Fall and Rise of Virtual Reality](http://www.theverge.com/a/virtual-reality/) and [Introduction to Virtual Reality](http://www.slideshare.net/marknb00/comp-4010-lecture-1-introduction-to-virtual-reality))

### The message of the medium

VR and AR expected to disrupt not only games and film, but also music, advertising, social media, education, travel, and who knows what else (maybe Black Mirror does). But do we know what we are doing?

> Palmer Luckey, Oculus CEO: "I think it will be VR content and software that will drive the industry long term". 

> Richard Marks, Sony Magic Lab: "Wild West... there are no established genres. You don't get that opportunity very often." 

> Michael Abrash, Chief Scientist, Oculus: "The future of VR lies in the unique experiences that get created in software, and if I knew what those would be, even in broad outline, I would be very happy." 

It is even obvious in the instability of terminology. Is it a game, a film, an experience, a simulation, a world, ...? Are we audience / viewer / visitor / cybernaut / immersant / player / user ...?

---

> "A new medium can suggest a multitude of approaches. In 1929, Dziga Vertov's [The Man with the Movie Camera](http://www.youtube.com/watch?v=z97Pa0ICpn8) catalogued possibilities for the evolution of film. From narrative structures to special effects, it shows *what cinema could have become*. Virtual reality occupies a similar historical moment--it is unformed and hence its possibilities seem unconstrained."

> "Although the artistic community has often been excluded from the development of new technologies, this situation is changing. Artists no longer sit on the sidelines eventually to become grateful users of borrowed tools but have become active in development, creating a disturbance in the field with new contingencies... A new medium like Virtual Reality challenges traditional conventions not because the participant wears a helmet or glove but because it suggests new relationships between the viewer and the viewed... Unfortunately, as the medium of virtual environments becomes more and more defined, different approaches will be ignored, abandoned, or forgotten as the medium coalesces into a mature form." - Douglas MacLeod, Director of the Banff Art and Virtual Environments Project, in the preface of Moser, Mary Anne, and Douglas MacLeod. [Immersed in technology: art and virtual environments. MIT Press, 1996.](http://mitpress.mit.edu/books/immersed-technology), emphasis added.

The above citation neatly echoes the aims of this course. In 2016, twenty years since this was written, and nearly fifty years since virtual reality's birth, it is finally becoming a widely-available consumer medium. 

> "...in these early stages of the VR lifespan, a common mishap is occurring: content creators shoehorning old formats into new technologies. As we explore this new medium, we are building on the backs of film, theater, narrative games and visual art to takes cues as to what to create in VR." [Will Virtual Reality and 360° Film Experience an Industry Divide?](http://vrscout.com/news/virtual-reality-and-360-film/)

Instead of shoehorning, what can earlier developments of cinema, game, performance and visual (and sonic) arts tell us about the the explosion of virtual reality (VR) in our imminent future? Theories, methods, and unique modes of expression have yet to be established by the collision of gaming technology with cinema. As with any emerging medium, a willingness to break rules, abandon habits and re-learn is necessary.

> "Hayao Miyazaki is [quoted](https://youtu.be/jtTBYMvLBbw) as making the following observation: "[anime] is produced by humans who can't stand looking at other humans.” I am proposing that this observation is not limited to anime. Much of what our culture makes seems to be made against, rather than for, other humans. Cities made for cars and rent extraction are simply a very large, indeed inescapable symptom of the and disease. We could say the same for industrialized art and music and nearly every globalized thing we come into contact with. The issue ... is the difference between the imagined, idealized vision, and its eventual commodification. For a brief moment, each new technology opens a space of freedom and opportunity. It is easy to mistake this space for something permanent. In reality, it is just the wave receding before the next tsunami. Whatever freedom can exist will have to be found on a different shore. For the creative person, this is adventure and excitement and possibility. But for everyone else, the tsunami can be devastating." - Marcos Novak

[![Codex](img/codex.jpg)](http://en.wikipedia.org/wiki/Codex_Seraphinianus)


### The illusions of VR

- Film/animation depends on a perceptual illusion -- [persistence of vision](http://en.wikipedia.org/wiki/Persistence_of_vision). This is easier to understand via animation: around 12-15 frames per second is enough for the brain to interpret as movement, but only when sequent images are plausible enough to be fused. *Plausibility* in this case is a function of neurophysiology and cognition. (Of course, cinema also depends on other perceptual quirks, such as the brain's acceptance of cuts in editing even though nothing like a cut exists in real life, the suspension of disbelief through non-human perspectives, and so forth.)

- Stereoscopic 3D (S3D) builds on another perceptual illusion. Presenting to each eye a viewpoint slightly displaced laterally emulates the *parallax effect* -- one of the most powerful visual cues to impart depth (distance). Again, this is dependent on the human body, and also requires very careful alignment. Some, though few, people experience discomfort due to discrepancies between the stereoscopic 3D depth cue and others that are lacking, such as vergence.

Virtual reality depends on both of these illusions, and others such as egocentric spatialized audio, and greatly benefits from wide field of view, high resolution, and other factors of **immersion**. 

- The crucial addition for VR is *head tracking*, which means we can present a coherent image regardless of what direction we face. This illusion breaks down if the delay between movement and image (motion-to-photon) is greater than a couple of handfuls of milliseconds, which underlies the need for high frame rates (90fps for current desktop models) to avoid nauseating "judder". 

- This illusion is greatly enhanced by *position tracking*: matching the lateral movements of the head as well as its orientation, so that you can look around, over and under things, and generally benefit from more kinds of depth cues we experience in real life, as well as further reducing the chance of nausea.

The result is that the viewer no longer perceives an image plane worn in front of the eyes, and instead perceives oneself being present in another world. Instead of an image moving in front of your eyes, the world appears as a fixed space in which you are moving your own head. (This also means that stereoscopic content can be as close as your nose, something that S3D cinema cannot normally achieve because of the limits of the frame).

- The combination of all the above can create a compelling immersive experience. Instead of an image moving in front of your eyes, the world appears as a fixed space in which you are moving your own head. Together with the qualities of content, this leads to the evocation of **presence**, the sense of actually being-there in the world; a continuous illusion of non-mediation. 

- But aside from presence, VR also maximizes interaction (the extent to which a user can manipulate objects and the environment of the system) and autonomy (the system's ability to receive and react to external stimuli, such as actions performed by a user). In that regard, convincing experiences created by **real-time simulations that support agency** -- the ability to take meaningful action in a world and discover meaningful consequences -- are just as essential.

### Nausea and Simulator Sickness

> Simulation Sickness is a syndrome, which can result in eyestrain, headaches, problems standing up (postural instability), sweating, disorientation, vertigo, loss of colour to the skin, nausea, and - the most famous effect - vomiting. It is similar in effects to motion sickness, although technically a different thing. Simulation sickness can occur during simulator or VR equipment use and can sometimes persist for hours afterwards... If VR experiences ignore fundamental best practices, they can lead to simulator sickness—a combination of symptoms clustered around eyestrain, disorientation, and nausea. - [Article on Gamasutra - by Ben Lewis-Evans on 04/04/14](http://www.gamasutra.com/blogs/BenLewisEvans/20140404/214732/Simulation_Sickness_and_VR__What_is_it_and_what_can_developers_and_players_do_to_reduce_it.php)

Simulator sickness involves three kinds of issues:

- Oculomotor
	- Headaches, fatigue, eye strain, can't focus
- Nausea
	- Sweating, salivation, can't concentrate, burping/stomach awareness
- Disorientation
	- Blurry vision, dizziness (with eyes open or closed), vertigo (24%)

Which is to say, *virtual worlds can be dangerous!* See this 1996 NBC special:

<div class="youtube" data-embed="O0arluK5zrQ"></div>

In fact simulator sickness has been known about since the earliest flight simulators of the 1950's, but is still not fully understood. It is clearly triggered by "cue conflicts", whereby what some parts of the visual system are reporting does not match what other sensory components (such as proprioceptive systems) are reporting. 

(Some researchers hope to alleviate VR nausea by galvanic vestibular stimulation, [for example the Mayo Clinic](http://ir.net/news/virtual-reality/124021/mayo-clinic-vr-nausea/), but as yet this hasn't convinced the industry. [See also this](http://uploadvr.com/vr-sim-sickness-combated/
).)

Some people are far more or less susceptible than others. It generally affects younger people less, and tends to reduce with increased exposure (getting your "VR legs"). People with a history of MS, alcohol/drug abuse, etc. also tend to be more susceptible.

> Around 5% of all individuals will never acclimate regardless how much they try to build a resistance to it meaning there is a confirmed minority of individuals who will never be able to us Virtual Reality as a mainstream product over their lifetime. - [Sim Sickness guide on Oculus forums](https://forums.oculus.com/viewtopic.php?t=170)

Since nausea/sim-sickness remains one of the greatest risks to virtual reality's success, it is essential to consider in the design of an experience. 

> Virtual reality’s biggest enemy is bad virtual reality.  – Palmer Luckey

> The fear is if a really bad V.R. product comes out, it could send the industry back to the '90s, – John Carmack

### Latency

To some extent this is a hardware problem -- and recent advances in VR hardware and drivers have come a long way to minimize the risk. However this still very deeply affects how we design our content, and is important to understand.

- Latency is how long it takes for a message to transmit. *Motion to photon* latency measures how long it takes for a change in head rotation to be reflected in a change in the image perceived. It should be *consistently* under 20 milliseconds to avoid nausea. Failing to do so can result in  sluggish or sloppy motion tracking, in which the world 'swims' around you. Even occasional hiccups will be experienced as a disturbing "judder" that is never experienced in normal life. The Oculus and Vive hardware now run at 90 fps and their drivers do some tricks to help keep latency down, but it also depends crucially on the content and quality of the software. 
	
- Anything that can potentially interrupt or slow down rendering, or delay the motion-to-photon pathway, has to be avoided to prevent nausea. The need for low-latency and high-framerate is one of the reasons why certain visual details and effects common in video games are eschewed in VR. When the world surrounds in you stereoscopy, geometry is often more important than screen-based post-processing. In particular, many effects popular in games are actually rendered across several frames -- this is simply not viable for VR. The Unreal VR template disables such effects by default. 

- A related issue is image persistence: a low-persistence image has a longer black interval between presenting frames, which reduces the smear/blur/ghosting when moving your head. This is mainly a display screen technology issue and largely resolved in current generation hardware. 

![Persistence](https://lh5.googleusercontent.com/bS3bZRKphnYPK1IAP7DwYN6e3Y_7y6-8RnHVutmm15S_wjzkf4M1vDR0OczN0kHx6PVd-10jd4vmhDFNhY0I18_31ovaKI2s6X_noyC9jk0AutfhEM4BIvnNyFjS6Q)

### Motion cue conflicts

The other major cause of nausea is motion cue conflicts, in which the movement portrayed by the images presented is not consistent with real motion of the body (or with an expected motion). This is almost the inverse of motion sickness, and appears to trigger a response in the body consistent with an assumption of being poisoned. Modern life has also brought to us another real-world parallel: 

> Imagine you’re on a train and look out the window to see a train leaving the station. As that train begins to move it creates an illusion of movement in your own mind and your brain’s likely conclusion is that the train you are on is actually moving in the opposite direction, that illusion is called “Vection.” Vection occurs when a portion of what you can see moves, and is one of the things that can lead to motion sickness in VR." - [5 ways to reduce motion sickness in VR](http://uploadvr.com/five-ways-to-reduce-motion-sickness-in-vr/) 

Any change of velocity (or rotational velocity, i.e. turning) is an *acceleration*, which imparts a physical force on the body detected primarily via the vestibular system. If such changes occur in the virtual world but are not mirrored in physical vestibular response (e.g. by navigating with a joystick rather than on a treadmill) nausea can very rapidly ensue. 

There are two important categories of motion cue conflicts to consider:
- Virtual motions initiated by the viewer with no physical correlate (**the locomotion problem**)
- Virtual motions not initiated by the viewer (breaking the **HMD-is-the-camera** rule)

### The camera is always head-mounted

It is helpful to **think of the HMD as the camera** into a virtual world that is aligned to the real world. (At [Weird Reality](http://artandcode.com/), I heard several speakers described the HMD as a 'head-mounted camera'). 

> The rendered image must correspond directly with the user's physical movements; do not manipulate the gain of the virtual camera’s movements. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

The golden rule for designers is that we must **never take away control of the camera from the viewer**, not even for a moment. This means no fixed-view cut-scenes or 'cinematics', no full-screen imagery, no lens and framing control, etc. Also no motion blur, depth of field effects etc. (still takes away viewer control). 

> One of the big challenges with VR storytelling lies within the constraints on camera movement forced upon us by this tiny detail called simulator sickness. Quick zoom in to focus on a detail – nope, not possible, you can’t zoom in VR. Nice dolly shot moving around the scene – be careful or the viewer might have a look at what he had for breakfast instead of comfortably watching your experience... the safest bet is not having continuous camera movement at all. - [The limbo method](http://uploadvr.com/introducing-limbo-a-vr-camera-movement-technique-by-the-developers-of-colosse/)

Since the immersant is free to look in any direction they choose, you need to make sure all directions are valid, potentially valuable, and that nothing essential will be missed because 'they were looking the wrong way'. 

It also means that **everything should be in-world**. Nothing should "stick" to the viewer's headset -- not even messages/menus, head-up displays, etc. 

> Maintain VR immersion from start to finish—don’t affix an image in front of the user (such as a full-field splash screen that does not respond to head movements), as this can be disorienting... Even in menus, when the game is paused, or during cutscenes, users should be able to look around. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

User interface elements are uncomfortable if they are stuck to the headset, better if they are transparent overlays that keep the world's orientation, and best if they are actually objects in the world. They could be:

- on walls ![walls](https://twentymilliseconds.com/screenshots/ui-walls-example.png)
- on objects, on screens in world ![screens](https://twentymilliseconds.com/screenshots/vr_typing_hud.png)
- or cockpit, 
- or floating over its subject ![coins](https://twentymilliseconds.com/screenshots/lucky/coins-ui.gif)


### When is camera movement OK?

> Our inner ear detects various changes in velocity, or accelerations, but it doesn’t detect constant velocity. Because of this, developers can have someone moving at a constant speed in a relatively straight line and the simulator sickness effects will be greatly reduced. - [5 ways to reduce motion sickness in VR](http://uploadvr.com/five-ways-to-reduce-motion-sickness-in-vr/)

There *are* plenty of examples of VR projects that also utilize moving cameras, but you can still look around independently on top of this motion. Generally the fixed component of the camera motion is slow, at constant speed, in a straight line in the world, or only in the direction the person is facing. This is the kind of "rails" experience that has been disappointing to many, and still nauseous to some. Senza Peso is an interesting example.

For more complex camera movements an option is to fade out all but the most important elements to minimize visual flow during the movement. This "limbo effect" was suggested by the authors of Colosse in order to allow non-nauseating camera effects -- and it also relies on some (fairly subtle) cues of ground and body orientation, as well as removing most of the elements of the scene to reduce vection (a method they also leveraged for narrative focus):

![colosse](http://uploadvr.com/wp-content/uploads/2015/09/limboBeachSnippet13.gif)

> Notice the subtle points of reference in the scene that are meant to maintain a consistent frame of reference. Somewhat like staring at a single spot on the floor to maintain balance. We used two elements to create this reference frame: a subtle particle effect and a ground plane far below the user. Using short lived particles we were able to create this artificial reference frame without distracting the user. - [Introducing Limbo, a VR camera movement technique by the developers of Colosse](http://uploadvr.com/introducing-limbo-a-vr-camera-movement-technique-by-the-developers-of-colosse/)

In contrast, one of the most disturbing camera motions of all is the oscillating 'head bob' and other 'camera shake' effects often added to games. (The head-bob in particular is right around a 3-5Hz frequency that is particularly nauseous.)

![headbob](https://lh5.googleusercontent.com/fUQXkmhrWdpCi_F9vfbI8U2Ss-zB5O11xn_wNCBbTSJczmjaRefoV26EflYqwgNpgK0hrgC4ZTB372IalQhssSKD98MZ7B8lp04glfqiXpFwICL5MuzlNPBzNaw3MA)

### Collisions

It is also disturbing to be suddenly (unexpectedly) moved in the world because of collisions with objects or other dynamic impacts. However if collisions do not stop camera motion, people will be able to simply walk through walls and poke their heads inside of objects in the world, and float rather than fall, etc. 
- To deal with collisions, some recommend simply fading the image toward black as you get very close to a object's surface, or enter inside of it. This is often enough to naturally guide people away from walls and other surfaces, and also prevents the disturbing vision of a wall made of paper, or the betrayal of the secrets behind it. 
- Similarly, to effect impacts, a dip to black around the movement is an option.

### The locomotion problem

Avoiding motion cue conflicts altogether would limit viewer's exploration of a virtual world to the same physical dimensions of the real room they are in. To explore vaster worlds we must allow people to move virtually but not physically, via **some design compromises that nevertheless minimize triggers of nausea**. [Some say that this locomotion question is the biggest problem for VR.](http://fatedblog.com/2015/08/06/locomotion-simulation-sickness-and-the-fear-of-vr/). 

> "It’s in the interest of all parties to keep faulty reality away from users, and Sony, Valve, and Oculus all have the quality control systems in place in order to do that. Oculus has even devised a new “comfort” rating system, which divides its launch lineup of games into “comfortable,” “moderate,” and “intense” categories." - [Virtual Reality’s Locomotion Problem](http://motherboard.vice.com/read/virtual-realitys-locomotion-problem?trk_source=recommended)

Even being able to turn around to face behind you (rather than looking over your shoulder) is problematic. The classic yaw movement -- a horizontal rotation -- has been described as "VR poison" by John Carmack -- but without it, our worlds will be mostly straight paths... This is really problematic for artists coming from gaming environments -- most people simply can't use mouse or right-stick to change body orientation in the world without becoming sick. 

> Remember that “acceleration” does not just mean speeding up while going forward; it refers to any change in the motion of the user. Slowing down or stopping, turning while moving or standing still, and stepping or getting pushed sideways are all forms of acceleration. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/) 

- Similarly, the kinds of lateral 'strafing' movements that are common to first-person shooter games can be quite disturbing in VR. 
- Moving over uneven ground can create unexpected vertical movements. Either steady the movement, or soften the ground.
- Stairs can be especially unpleasant (both going up and down). Use elevators, or ramps with very shallow inclines.

Finally, walking around a world is really counter intuitive, as you may have to think about two different spaces simultaneously -- the space you can physically move around in, and the space you can navigate around in. 

### Some solutions / compromises

**Just don't do locomotion at all** 

- Create a world that is sufficiently interesting at the scale of a small room
	- E.g. Job Simulator, Fantastic Contraption, I expect you to die, etc.
	- E.g. create a world at a 'tabletop' scale
- Create a world that changes over time around you
	- E.g. Sightline - The Chair
	- E.g. work with scale, zooming into detail or out to macroscopy, rather than change in location

A lot of 2016's sanctioned VR content avoided locomotion.

**Instant accelerations are better than smoothing**

- If you have to change velocities, do it instantaneously rather than gradually. (This differs from the norm in screen-based games, for example). Don't accelerate smoothly: immediately moving and immediately stopping is better (for most people).
	- Same for rotations. Jumping between angles is better than smooth panning (for most people). Cloudhead games calls this "comfort mode": snapping a predictable number of degrees left or right, and holds that this significantly reduces nausea. But for some players this breaks immersion too much, and it can also leave immersants a bit confused as to where they are actually facing.

<div class="youtube" data-embed="Gp0eMNSVtZA"></div>

**Instant transitions (AKA teleport)**

Vive's introductory content The Lab uses this method extensively. It is very low in terms of nausea, but relatively immersion breaking. It also depends on developing a method to identify valid locations to teleport to. 

[It has been argued that we can handle teleports in VR in a similar way that we can handle cuts in TV](https://www.engadget.com/2016/10/07/why-teleportation-makes-sense-in-virtual-reality/)

<div class="youtube" data-embed="nmR8iqXSspA"></div>

It can also become a game mechanic:

<div class="youtube" data-embed="gbp7xX9QPOc"></div>

Unreal's VR template includes teleport support.

**The third person view**

- Many developers have suggested a 3rd person (behind the avatar) viewpoint reduces the nausea. Oculus bundled a 3rd-person platformer ("Lucky's Tale") with the first release.

- A rather more unusual mode of navigation switches into 3rd person while moving, and back to 1st person when stationary.

**Reducing the field of view during motion**

Sim sickness is much less prevalent when the field of view is lesser, however this also reduces immersion & presence. Some suggest reducing FOV only in those moments that could be particularly nauseating. Others have suggested a kind of small FOV preview overlay while moving, that expands out to full screen when movement ends.

<div class="youtube" data-embed="lHzCmfuJYa4"></div>

Reducing the field of view may work because it reduces vection.

**Anchoring (Cockpit) methods**

Placing a reference frame around the point of view can help stabilize the senses -- which is why cockpit-based simulations (inside cars, spaceships, robots, or even just a helmet, etc.) can handle much greater accelerations and rotations without inducing sickness. It might be as simple as having a reference that says which way is "body-forward", but it also taps into the reduced field of view as above.

[However it might be possible that the reference frame is semi-transparent, and even that it is not present for much of the time.](https://www.reddit.com/r/oculus/comments/3yihao/i_solved_vr_sickness_maybe/) -- more research is needed. See also the "canvas mode" [here](http://tore-knabe.com/virtual-reality#MovementExperiments)

**Give them a body?**

Many people report it disturbing to look down and see no body, especially for sedentary experiences. This may be related to giving a reference frame that has a *logical* anchor in the world. However, some say that looking down and seeing somebody else's body is equally disturbing, and others have shown that even a reference frame with no ontological sense can help. More research needed!

> A virtual avatar ... can increase immersion and help ground the user in the VR experience, when contrasted to representing the player as a disembodied entity. On the other hand, discrepancies between what the user’s real-world and virtual bodies are doing can lead to unusual sensations (for example, looking down and seeing a walking avatar body while the user is sitting still in a chair). - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

![nose](http://www.wired.com/wp-content/uploads/2015/04/vrnosetuscany.gif)

[Research at Purdue suggests that overlaying the peripheral image of a nose helps reduce simulator sickness by 13.5%](http://www.wired.com/2015/04/reduce-vr-sickness-just-add-virtual-nose/)

Again, a non-realistic body might be better than a pseudo-realistic body. Perhaps it need not even be human (or humanoid). This removes issues of mismatch size, gender, skin color, age, etc that could create cognitive dissonance. Alternatively, give immersants control over their avatar appearance.

> When it comes to modeling player avatars in VR, abstract trumps the real. Malaika says Valve has found that players tend to feel less immersed in games that try to model hands realistically, and more immersed in games with cartoony hands. - [Valve advice for VR](http://www.gamasutra.com/view/news/250362/Valve_shares_advice_on_designing_great_VR_game_interactions.php)

**Redirected walking**

The notion here is that while walking in the real space, the virtual world is slightly rotated (below perceptual levels). Although we feel we are walking in a straight line in the virtual space, we are in fact walking in circles in the real world. Problem: still requires much larger spaces than most rooms.

<div class="youtube" data-embed="KVQBRkAq6OY"></div>

A related method is 1:X motion, in which moving 1 meter in the real world may move you more than 1 meter in the virtual space. Horizontal exaggeration appears to not induce nausea, but vertical movement should remain 1:1.

<div class="youtube" data-embed="At_Zac4Xezw"></div>

**Displacement of motor functions**

Disturbance is reduced if some body actions accompany a movement. Some games use a 'running in place' or 'paddling with the hands' behaviour to trigger walking in the virtual space:

<div class="youtube" data-embed="15lvlAEHXww"></div>

Or swimming etc.:

<div class="youtube" data-embed="MjwNItck_Vg"></div>

Or grappling hooks:

<div class="youtube" data-embed="3Ore5DG1qT0"></div>

Other experiences use the direction of the hands or fingers to indicate direction of motion, which appears to reduce nausea.

---

Overview of locomotion methods:

<div class="youtube" data-embed="p0YxzgQG2-E"></div>

[Another overview here](http://ignite-vr.com/blog/2016/09/24/locomotion-in-vr/)

Many of these solutions are utilized in EagleFlightVR, which has had [very strong reviews commenting about the lack of nausea](http://www.roadtovr.com/eagle-flight-review-vr-psvr-htc-vive-oculus-rift/).

<div class="youtube" data-embed="4TJdTB5qQjA"></div>

### Other forms of disturbance

> Avoid visuals that upset the user’s sense of stability in their environment. Rotating or moving the horizon line or other large components of the user’s environment in conflict with the user’s real-world self-motion (or lack thereof) can be discomforting.  - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/) 

***Spatial***

- The head-height above ground should be consistent with the immersant's own height, whether sitting or standing. 
- Real-world movement is more comfortable. Humans walk at ~1.4 meters per second (this is much slower than 'walking' in most video games).
- Objects drawn from the real-world should have consistent and usually accurate scale.
- On the other hand, miniature worlds work well -- about table-sized + 3rd person view
- Avoid confined spaces.
- No image-based effects such as particles, as they can look flat and break stereoscopy.

**Lighting & texturing**

- Avoid very bright lights, flickering lights, and areas of high contrast -- especially in peripheral vision.
- Avoid flicking and flashing, especially in peripheral vision.

> Refrain from using any high-contrast flashing or alternating colors that change with a frequency in the 1-30 hz range. This can trigger seizures in individuals with photosensitive epilepsy. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

- Avoid untextured surfaces, as the lack of detail provides less distance cue and weakens the perceptual illusion, making other conflicting signals more problematic.
- On the other hand, avoid high-contrast textures, which are more likely to cause flickering due to aliasing noise.
- Avoid textures that are obviously repetitive, like tiling patterns. Any high-spatial frequency repetition can give discomforting perceptual signals. They can also trigger photosensitive epilepsy.
- For the same reason, avoid very thin objects, and avoid very regular or straight objects -- irregular/random/organic shapes are more comfortable.

> The images presented to each eye should differ only in terms of viewpoint; post-processing effects (e.g., light distortion, bloom) must be applied to both eyes consistently as well as rendered in z-depth correctly to create a properly fused image. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

**Uncanny content**

> VR is an immersive medium. It creates the sensation of being entirely transported into a virtual (or real, but digitally reproduced) three-dimensional world, and it can provide a far more visceral experience than screen-based media. Enabling the mind’s continual suspension of disbelief requires particular attention to detail...  - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

The closer we get to experiences we have every day (e.g. walking), the higher the risk of creating perceptual cues that do not match reality. This may be related to the *uncanny valley*. Characters not looking at you / not responding to you properly can be particularly disturbing.

More abstract worlds are less likely to cause such conflicts; non-photorealistic environments in many ways have advantages. Overly realistic environments can also confuse immersants -- who may begin to expect that *everything* in the environment can be interacted with, and be disappointed when it isn't. 

Alternatively, let all things be interactive:

<div class="youtube" data-embed="_TPXop3ONPk"></div>

**Muscle fatigue**

> People will typically move their heads/bodies if they have to shift their gaze and hold it on a point farther than 15-20° of visual angle away from where they are currently looking. Avoid forcing the user to make such large shifts to prevent muscle fatigue and discomfort. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/) 

Keep most content at a comfortable viewing angle. It is uncomfortable to look up or down for very long, or to twist sideways frequently or for sustained time. 

> Don’t require the user to swivel their eyes in their sockets to see the UI. Ideally, your UI should fit inside the middle 1/3rd of the user’s viewing area; otherwise, they should be able to examine it with head movements. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

And if you expect people to sit through the experience, remember that they will only rarely (if at all) see things behind them.

**Experiment**

Try other ideas out. Try them out on lots of people. Just because it feels OK for you doesn't mean it will for others -- and this is more true the more time you spend in VR.

Give people the option to control the intensity of effects that can induce nausea. Not everyone wants to be limited, and some players are willing to forego comfort (or simply are less susceptible to the nausea); recent examples of succesful games that 'break the rules' in this way include Onward and Climbey.

**Learn how to avoid it**

Hardly a solution, but there are a few techniques that people susceptible to sim sickness can make use of: 

- Take time to calibrate the headset to your eyes -- your inter-pupillary distance, your field of view, the height of your eyes above ground (when standing), etc.
- When turning, keep your eyes locked on to a specific point. Also, focus on the horizon in moments that you feel unsteady.
- Close your eyes for any nausea-inducing moments.
- Sitting is usually better than standing, so long as the experience can place you at an appropriate height in the virtual world. Some prefer lying on their backs.
- Remember to take breaks.
- Don't expose yourself to nauseating experiences too often -- it can make you more sensitive, and create negative associations that are hard to shake (e.g. with the smell of the headset).
- On the other hand, over time the effect can reduce. Early pseudo-3D game such as Doom and Duke Nukem, at very low resolutions on screens, were still able to evoke motion sickness in players -- this seems remarkable and difficult to believe today -- but it suggests that perhaps VR experiences will be less nauseating the more we are used to them.

> When a land-lubber steps onto a boat for the first time, often the rocking and variations in vestibular motion from the ocean causes a feeling of ‘sea-sickness’ that is not too different from simulator sickness. However, for most people, after a few hours or days that feeling typically dissipates as they get what is commonly referred to as their ‘sea legs.’ It is something that experienced seamen are very well adapted to. It is also something, I would argue, that replicates itself in VR. - [5 ways to reduce motion sickness in VR](http://uploadvr.com/five-ways-to-reduce-motion-sickness-in-vr/) 

- Do it in a well-ventilated space, at a comfortable temperature.
- Eat ginger (a long known remedy for motion sickness). Some also recommend a little alcohol, while others say that this makes it worse. Do not try VR when sick, hungover, etc. 
> A popular household remedy in Asia is rub eucalypti leaves together and inhale the scent produced from them. - [Sim Sickness guide on Oculus forums](https://forums.oculus.com/viewtopic.php?t=170)

---

### Space and the body 

- We cannot focus on objects closer than ~5cm, for some people as much as 20cm, so it is good to avoid placing virtual content too close to the head. And in general it is disturbing for objects to intersect the body (whether a virtual body exists or not). Static objects are usually fine as most people will naturally move around them, but dynamic objects may need to be aware of where the person is.

> Converging the eyes on objects closer than the comfortable distance range above can cause the lenses of the eyes to misfocus, making clearly rendered objects appear blurry as well as lead to eyestrain. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

- Shocking/scary etc. content is much more powerful in VR, as it can approach the body and trigger physiological responses in a way that past media could not. For example, too much action of flying bullets, explosions, moving vehicles etc. around the user can be distressing, where it would be quite acceptable in a screen-based film/game. For good reason a lot of early VR experiences are in the horror genre. Compare the slow-motion time of the Showdown demo.

> When we started tinkering with the DK1 back in the beginning of 2014, the VR scene was pretty much two things: first-person view horror games and rollercoasters. A lot of people saw the future of VR entertainment as that kind of experiences. - [Locomotion and the fear of VR](http://fatedblog.com/2015/08/06/locomotion-simulation-sickness-and-the-fear-of-vr/)

- You may need to consider the physical limits of the space in which tracking works (and in which it is safe for a blind person to move around). Again, the hardware's drivers are beginning incorporate features to automatically show the tracking limits (e.g. the blue grid that becomes visible in the Vive).

> Provide the user with warnings as they approach (but well before they reach) the edges of the position camera’s tracking volume as well as feedback for how they can re-position themselves to avoid losing tracking. We recommend you do not leave the virtual environment displayed on the Rift screen if the user leaves the camera’s tracking volume, where positional tracking is disabled. It is far less discomforting to have the scene fade to black or otherwise attenuate the image (such as dropping brightness and/or contrast) before tracking is lost. - [Best practices, Oculus](https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/)

- 3D depth perception is extremely powerful at short range, and effective within a range of a few meters. Beyond 10 meters, stereopsis ceases to be the most important depth cue, and parallax (i.e. moving via position tracking), texture, size, lighting etc. take precedence. It therefore makes sense to place significant content and interaction in this range. For this reason, user interface overlays are usually positioned 1-3 meters away. Several HMDs have their optics (lenses etc.) designed to make it most comfortable to view virtual objects within a handful of meters. 

- [Beyond around 60m](https://forums.oculus.com/viewtopic.php?f=33&t=4155),  there is virtually no distinguishable stereopsis effect at all, and it can be effectively rendered in mono (may be more efficient).

- Spatializing audio is much more important -- presenting audio in mono, or worse, in a single speaker, breaks immersion. Headphone audio should also use head orientation, and located sounds should get significantly louder when you lean toward them closely. 

- Until headsets go wireless (perhaps 2017) the tethering cable to the headset can limit certain motions.

- Physical interaction devices can't been seen while wearing a headset. The keyboard in particular is almost impossible to use. 

> “In VR, you don’t have a keyboard full of hotkeys,” says Malaika. “The buttons on a controller are much more limited, so you have to think about how to provide the same number of choices…and manage the number of choices a user has.”   - [Valve advice for VR](http://www.gamasutra.com/view/news/250362/Valve_shares_advice_on_designing_great_VR_game_interactions.php)

- Even with tracked hand-held devices, or tracking via Leap Motion or the Kinect, there is still no haptic feedback -- no sense of touch. For some reason, gloves are not in fashion for VR this time around (as they were in the 90's). For Leap Motion in particular, [here are some accumulated best practices.](https://developer.leapmotion.com/assets/Leap%20Motion%20VR%20Best%20Practices%20Guidelines.pdf). There has been some research into using ultrasonic haptic displays, but the effective range is low. The state of the art in touch is to put physical objects in space:

<div class="youtube" data-embed="NSCZxsd-9hA"></div>

**See also:**

(Elements borrowed from [Kevin Burke's guide](https://kev.inburke.com/slides/virtual-reality/), [Simulator Sickness](http://www.gamasutra.com/blogs/BenLewisEvans/20140404/214732/Simulation_Sickness_and_VR__What_is_it_and_what_can_developers_and_players_do_to_reduce_it.php))

[Tips from a team who ported a base-jumping game to VR](https://youtu.be/DqZZKi4UHuo?list=PLckFgM6dUP2hc4iy-IdKFtqR9TeZWMPjm&t=228)

See the [Simulator Sickness questionnaire](https://www.twentymilliseconds.com/html/ssq-scoring.html)

- [Valve advice in interaction in VR](http://www.gamasutra.com/view/news/250362/Valve_shares_advice_on_designing_great_VR_game_interactions.php)

-------

## Introduction to Unreal

Unreal has [long been used in many "AAA" (large-scale/high-production value/high-budget) games](https://en.wikipedia.org/wiki/List_of_Unreal_Engine_games), but was open-sourced with an amenable license in early 2015; and now has built-in support for VR, including the Oculus Rift, HTC Vive, and OSVR. Some better-known games created in Unreal include:
Assassin's Creed, Batman, Bioshock, Borderlands, Eve: Valkyrie, Final Fantasy, Hellblade, Kingdom Hearts, Mass Effect, Mirror's Edge, etc. A significant percentage of experiences available for the Oculus Rift and HTC Vive were authored in Unreal.

### Some quick inspiration

On Youtube there are now plenty of examples of high-speed video captures of level design using game engines such as Unreal, e.g.:

<iframe width="640" height="360" src="https://www.youtube.com/embed/UCOww2sdsTM?rel=0" frameborder="0" allowfullscreen></iframe>

<iframe width="640" height="360" src="https://www.youtube.com/embed/-l7ia1M1r1E?rel=0" frameborder="0" allowfullscreen></iframe>

### Getting Unreal

You can [download Unreal from here](https://www.unrealengine.com/what-is-unreal-engine-4), it just needs an email registration. However Unreal needs [a reasonably fast PC or Mac](https://docs.unrealengine.com/latest/INT/GettingStarted/RecommendedSpecifications/#recommendedhardware) to run comfortably. Mac users will also need a mouse with at least two buttons. It also will eat up gigabytes of disk space!

- Go to [https://www.unrealengine.com](https://www.unrealengine.com) and follow the "Get Unreal" link to register an account and then download the launcher application. 
- Run this launcher (you will need to sign in again), and then under the "Unreal" tab download the Unreal Engine. This will take a while!
- From the launcher also get the "content examples", and perhaps any other of the free samples you find interesting. Warning -- these examples can eat even more *gigabytes* of disk space!! 

Once Unreal is installed, we can create a new project. Open the editor directly, or via the launcher. It should open with the 'project browser' / 'new project' window. 

- In the ```New Project``` tab:
	- Choose the Blueprint tab (it's easy to switch to C++ if you need to later)
	- For now, choose the "First Person" template (we can migrate to a Virtual Reality template later)
	- Choose ```Desktop``` target, ```Maximum Quality```, and ```With Starter Content```
	- Set the project path (e.g. ```E:\Unreal Projects``` )
	- Set the project name
	- Press 'Create Project'
	- After a few moments, the new project will open in the editor
	
In future, you can go ahead and open your existing project from the launcher, or the editor's project browser. 


> See also the [Level Designer Quick Start](https://docs.unrealengine.com/latest/INT/Engine/QuickStart/index.html) documentation.

### Basic concepts

See [Getting started with UE4](https://docs.unrealengine.com/latest/INT/GettingStarted/index.html?utm_source=launcher&utm_medium=ue&utm_campaign=uelearn).

You use the *Unreal Editor* application to author your world. Ultimately you will export it as a distributable application (.exe on windows), but most of the time you will edit and also playtest the world via the Editor. 

A *Project* is actually a whole folder of files on your disk, including a ```.uproject``` file that keeps the global settings together. You also see these files via the Content Browser in the Unreal editor. Each project has one or more **Levels** (sometimes also called **Maps**), which are independent 3D scenes or locations into which you place objects to define the world experienced. Each level is saved as a separate ```.umap``` file.

> I recommend that you do all your development in a single project. You can create new levels to try different ideas out, but keeping it all in a single project will make it easier to copy items between levels, and reduce disk space required. There are also methods to 'teleport' between levels to build a larger world.

Any object (a player, character, geometry mesh, light, etc.) that is placed in 3D space (with position/translation, rotation/orientation, scale etc.) is an **Actor**. A *Pawn* is a special kind of actor that serves as an avatar or persona. **Assets** (in the content browser) are the resources you can place into the world, while Actors are instances of resources. There can be many instance Actors of a single Asset in a world. Modifying an asset will change all the actors based upon it. At its most basic level, creating levels boils down to placing Actors in space. Actors can also be created dynamically ("spawned") by scripts, e.g. the arrows launched by a bow. Actors may be made of several **Components**. A Component is a piece of functionality that can be added to an Actor. Components cannot exist by themselves. For example, a Spot Light Component will allow your Actor to emit light like a spot light.

> In summary: A Project has many Assets, including multiple Levels (maps). Each Level (map) has a number of Actors, based on the Assets, and modified with instance-specific parameters such as location. Actors can come in different kinds, and may include a number of Components.

### The editor panels

Watch the [video tutorial here](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=w4XlBKeE46E).

The main panel is the 3D **viewport**, showing a preview of the world. You can navigate around the world and also modify objects (actors) in here.

Above the viewport is a *toolbar* with which you can trigger builds and preview (run) your project in different ways, including within a VR headset.

On the left, the **Modes panel** selects different task workflows, such as placing objects, creating geometry, painting meshes, generating trees, etc. Mostly we stay in the initial "placement mode".

At the bottom, the **Content Browser** is where you can manage the *Assets* used in your project. With this you can also drop any existing assets into the scene. You can also double-click on some items in this browser to modify them in special editors.

On the right, the **World Outliner** panel shows all the *Actors* involved in the current level. You can select them here, or in the 3D viewport, to modify them. You can also use this to "parent" one object to another, so that they always are positioned together (drag one item onto another to set it as parent). Below that, the **Details panel** is used to modify properties of whatever actor is currently selected, and also to add additional components to them.

> Tip: Hold Control+Alt and mouse-over any Editor feature to see a help window about it.

### Viewport navigating

See the [documentation](https://docs.unrealengine.com/latest/INT/Engine/QuickStart/2/index.html) or the [video tutorial](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=j2CKS6G3G2k). Quick notes:
- **Left mouse**: "walk mode" move forward/backward/turn... 
- **Right mouse**: look around
- **Both buttons** (or middle button): vertical & strafe movement
- **Mouse-scroll** to zoom in and out (not the same as moving!!)
- Keys: with any mouse button (usually the right) held down, keys **W, A, S, D, Q and E** move around. **C and Z** will do a temporary zoom in/out.
- Select an object by clicking on it, then press **F** to focus on it (or, double-click an item in the World Outliner)
- **Alt+left mouse** ("maya style") pivots around the focused object.
- **Alt+right mouse** can dolly toward/away from the object.
- Press **G** to turn nonvisible items (e.g. lights) on and off

### Placing objects 

A new level begins fairly empty. Ways of adding content ([see tutorial](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=01BL2bWeJSA)):

1. We can drag basic objects & actors from the 'place mode' palette at the left. You can drag objects from the browser into the viewport, and also materials right onto objects in the viewport. And also more abstract items such as lights, triggers, and other classes. 
2. We can drag objects from the Content Browser below.
3. Right-click in the viewport and choosing 'place actor', then selecting the type to add.
4. **alt-drag** to make a copy of an actor

### Spatial transforms

Once placed, objects can be moved [(see tutorial)](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=wHJCv-Ph6zU), rotated [(see tutorial)](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=ralshA7aWag), and scaled [(see tutorial)](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=rqJoX8orsUM). Quick tips:

- Press **w** to enable the move tool. The red, green, and blue arrows represent the X, Y, and Z axes respectively. Click & drag an arrow to move the object.
	- Press the **end** key with an object selected and it will drop down to sit on the floor.
	- Press **ctrl end** to snap an object to the current snapping grid.
	- Hold down **shift** while moving an object, and it will also move the camera at the same time.
- Press **e** to enable the rotation tool. Click on one of the arcs to start rotating, and it will show you how much you are rotating in degrees. 
- Press **r** to enable the scale tool. To scale in one axis choose the corresponding red, green, or blue box, or to scale in all axes grab the white cube in the center.

The top-right toolbar shows which tool is enabled. 
- The globe icon (after the scale tool) chooses between world-space and object local-space to move & rotate in. (**Ctrl ~**). 
- After this there are icons for setting the snapping options for each of move, rotate, and scale. Snap to grid is useful for creating buildings -- making sure there are no gaps between walls!  The **[** and **]** keys change the snapping size.

Position, rotation & scale can also be modified numerically from the object's details panel. Unreal measures in centimeters, and the Z axis points up.

Press **ctrl g** to group items, **shift g** to ungroup. Grouping is helpful to move a bloc of items together en masse.

### Hiding the First Person Template's person

- In the Content Browser navigate to Content/FirstPersonBP/Blueprints and double-click the "First Person Character" to open the blueprint editor
- Switch over to the Viewport tab, and select the gun
- In the Details panel, type in "owner" in the search box, and then check "Owner No See" underneath.
- Do the same for the arms
- Save and close

- Now in the Content Browser navigate to Content/FirstPersonBP/Blueprints and double-click the "FirstPerson GameMode" to open the property editor
- In the "HUD Class" drop-down, choose "None"

### Managing Assets

Some assets are created within the Unreal Editor, others you import from outside. All the assets in a project can be found in the Content Browser [(tutorial here)](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=tG2KML-CDRo), which mirrors the directory structure on disk. Some of these are files that can be edited externally (e.g. editing texture images using Photoshop, or mesh geometry in a 3D modeling software) -- when these files are modified externally, Unreal will automatically re-import them into the world.

You can use the **add new** button to create new simple assets or content packs. You can **import** external assets to the content browser. Assets can also be duplicated, and many can be edited *before* bringing them into the level. Double-click the asset and it will open a special sub-editor in which it can be modified.

Projects can end up with hundreds of assets. **It is essential to create meaningful names for assets, and place them in organized folders in the content browser**. There is a search tool and options to filter assets by type. You can also open multiple content browser panels, and lock them to different sub-folders of your project.

### Getting more assets

**From other Unreal projects**. The easiest place to get more assets (more than what the starter content pack gives you) is to grab them from other Unreal projects. There's a very generous set of example projects available through the Unreal Launcher's Library tab, for example. Once these are downloaded you can open them up, navigate through their Content Browsers and see if there's anything you want to take. Getting them into your own project however takes a few steps, and is called *migrating assets* [(see the documentation here)](https://wiki.unrealengine.com/Migrate_content_between_projects). Migrating an asset will also migrate everything that it depends on (textures, materials, blueprints, etc.):
1. In the source project, locate the content in the Content Browser, right-click and choose "Asset Actions > Migrate"
2. Confirm all the assets to migrate
3. Locate the project "Content" folder (on disk) of the destination project to migrate to (not a subfolder!)
4. Now those files should also appear in the destination project. You may want to move them into appropriate subfolders of your project now. 

**From the Unreal Marketplace**. The Unreal Launcher's Marketplace tab also has a range of assets that can be downloaded (though most are not free). At the time of writing, some decent looking free packages include:
- [Epic games content](https://www.unrealengine.com/marketplace/profile/Epic%20Games)
- [architectural visualization scene](https://www.unrealengine.com/marketplace/lightroom-interior-day-light) and [here's another](https://www.unrealengine.com/marketplace/xoio-berlin-flat), but might not be compatible with Unreal 4.14
- [materials](https://www.unrealengine.com/marketplace/gametextures-material-pack)

> Between the above built-in free sources for Unreal-specific content there is over 40 gigabytes of objects, landscapes, characters, materials, effects, sounds, and so forth to draw from!


**From anywhere else.** Assets can also be imported from other sources. For sounds, images, videos, 3D models etc. any compatible format file can be simply imported to the Content Browser. 

Below are some other places where free 3D models can be downloaded. Depending on what your intended use for the project is, pay special attention to the licensing terms of these models -- this is intellectual property.

- [Google 3D Warehouse](https://3dwarehouse.sketchup.com/?redirect=1)
- [3D models for free](http://tf3dm.com) 
- [Turbosquid](http://www.turbosquid.com/Search/?KEYWORD=Free)
- [Exchange 3D](http://www.exchange3d.com/Free%203D%20Models/cat_35.html)
- [3DVia](http://www.3dvia.com/search/?search%5Bquery%5D=free&search%5Bresults_per_page%5D=12&search%5Bsort_order%5D=Rank&search%5Bfile_types%5D=1)

[This plugin](https://www.unrealengine.com/marketplace/playup-tools-plugin-for-ue4) lets you import scenes from SketchUp's PlayUp.


### Creating Geometry

A *Geometry Brush*, also sometimes called "BSPs" [(see documentation)](https://docs.unrealengine.com/latest/INT/Engine/Actors/Brushes/index.html), is a great way to start filling in and carving out volumes of space in a level. They can be used to rapid-prototype a level where static meshes have not yet been created. 

In the Placement mode tab, choose the BSP (also known as *geometry brush*). Now we can drag different kinds of brushes into the level, such as boxes, cylinders, and stairs (though remember -- stairs can be nauseous in VR!). After dragging it in we can resize it. Be careful to use a snapping grid to make sure geometry snaps to tight alignment, and set the size accordingly (e.g. 10cm may be too small for laying out buildings!). 

Using the **Geometry Editing Mode**, very simple 3D modeling can be performed, modifying the faces and vertices (corners) of objects.  

**Creating holes and indentations**: A geometry brush's settings can be set to "Subtractive" type, which will carve out holes in other brushes -- however the order of brushes is important. For example, to create an opening in a wall, we can duplicate the wall, reduce its size, set its brush type to "subtractive" (in the details panel), then drag it back into the wall. 

**Updating lighting etc.** Hit "Build" in the top toolbar to apply the changes, and rebuild lighting models etc. This can be especially important when building enclosed spaces with holes for light to come through.

**Applying materials**: You can simply drag & drop materials from the Content Browser onto geometry brushes. By default, materials will drag onto a single face. To drag onto all faces of a geometry brush, first press "shift+b" to select all the faces (or, if you select a wall, pressing "shift+w" will select all adjacent walls).

**Aligning materials/textures (to avoid seams):** Select all adjoining panels (with "ctrl") and in the details panel, set Geometry / Alignment / Align surface planar to make sure there are no seams between them.

**Adding feature and prop meshes, lights, etc.**: After adding the brushes and dropping in materials, we can start adding some more detailed meshes, improve the lighting, etc. The starter content has a few good architectural items to work with, such as a "glass window" panel that can be used for all kinds of glass panels.

---

### Levels

You can make a new level at any time from File->New Level. 

To set the default level (the one that opens when a project launches), go to Project Settings

**Copying actors between levels (maps) of the same project**: Just select what you want to copy in the Outliner and Ctrl-C to copy, then switch to the destination level, and Ctrl-V to paste.

To set where the immersant will start a level (and what direction they face) when the level runs, move and rotate the "Player Start" actor. If you don't have a Player Start actor in the level, you can drag one in from the starter content collection. However it's common while editing a level to remove the Player Start, so that it will run from wherever you where editing from. 

### More advanced viewport options

**[Orthographic views](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=RoiQOwCg-4Q):**

In the top-right corner of viewport there is a small 'min/max' button which will alternate between a maximized 3D view, and a 4-up set of views for top, left, and front "orthographic" (non-perspective) views as well as the regular perspective one. Right-click & drag to move in these views, and mouse-scroll to zoom.

**[View modes & show flags](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=7UjP6gr44dc)**:

By default the viewport uses full lighting ("lit") -- other modes are unlit (raw materials), wireframe (raw meshes), detail lighting (lighting effects only), reflections, etc. Next to this option is the **Show** drop-down, where you can enable and disable different kinds of content in the viewport.

**Selecting translucent objects**: The "t" key will enable/disable the ability to select translucent objects. If it is off, you will select whatever object is seen *through* the translucent object.

**We can [lock the viewport to an actor](https://wiki.unrealengine.com/Videos/Player?series=PLZlv_N0_O1gasd4IcOe9Cx9wHoBB7rxFl&video=Z7OqNjFpy6U)** by right-clicking on an object and selecting the "pilot" option. This fixes our viewport's perspective to the object itself, and we can use the standard viewport movement controls to reposition the object. This is especially useful for positioning lights and cameras, for example.

**Viewing the Frame rate**: Command+Shift+H (or the dropdown top-right of the viewport)

---

### Landscaping

[See documentation](https://docs.unrealengine.com/latest/INT/Engine/Landscape/QuickStart/index.html).

Unreal has built-in tools for creating rich, undulating landscapes. These are ground surfaces that can have a broad variety of different features. Technically they are achieved by "height maps" -- for each XY position in space, there is an associated height value, just like geographical elevation maps. Areas can be flat, shallowly or steeply sloped; the only restriction is that they cannot overhang (though there are ways to deal with that if needed, see below). A level can have more than one landscape, if needed.

To create or edit a landscape, in the Modes panel select the **Landscapes** mode. 

If this is the first time to use the Lanscape mode, a green grid will show in the viewport. You can drag on the landscape's edges to make it cover more or less area. The default is huge -- for VR we don't want to travel such large distances. Reducing the number of sections to 2x2 or 4x4 might be enough.

> For recommended landscape sizes, see https://docs.unrealengine.com/latest/INT/Engine/Landscape/TechnicalGuide/index.html#recommendedlandscapesizes

It's a good idea to assign a landscape material before creating the landscape itself. The starter content doesn't have a good landscape material, but you can either create a new one or migrate one from another project. Creating landscape materials is a little more complex ([there's a tutorial here](https://www.youtube.com/watch?v=tsXVP0fykBM)), but we can migrate some from other example projects or content packs available through the Epic community (via the Launcher app). For example: 
- open the LandscapeMountains project, 
- navigate in the content browser to Content/Landscapes/Materials, 
- right-click on M_LandscapeMaterial_Inst, 
- choose Asset Actions > Migrate...
- click OK to select all dependencies
- navigate to & select your project's Content folder on disk (e.g. E:/UnrealProjects/MyProject/Content) & OK
- back in your project, locate the /Assets/Landscape/M_LandscapeMaterial_Inst in the content browser, and drag it into the Material option of the Landscape Mode's New Landscape panel.

Then "Create New" to instantiate the new landscape. Once instantiated, you can start to Sculpt and Paint it.

#### Sculpting

With the default sculpt tool, left click to increase elevation, shift-click to decrease. Use the brush options to change the size & strength.

Under the Sculpt tool menu there are many other tools: smooth, flatten, ramp, erosion, hyrdo erosion, etc. Smoothing & flattening is good for VR, as it feels more comfortable to walk on. Flattening can also be good for making rivers and roads.

Make sure that your Player Start object (if you have one) is above your terrain or else you will fall through the world. If this does happen, drag the player to start up on top of the terrain and hit the End key for it to start on the ground. Bear in mind that if a slope is too steep, the character will not be able walk up it.

#### Creating landscape from existing data

[See tutorial](https://docs.unrealengine.com/latest/INT/Engine/Landscape/Custom/index.html)

A new landscape can be created from an existing height map image by choosing "Import from File" instead of "Create New" when creating a landscape. The file needs to be in a specific format, which you can achieve via PhotoShop:
- set Image/Mode/16-bit
- set Image/Mode/Greyscale
- Export as PNG

Any landscape layer can be exported to a file, edited, and re-imported.

In Sculpt mode, right-click on Heightmap and choose Export to File. Once you have done this, you can edit the PNG elsewhere. Then right-click on the Heightmap and choose Reimport or Import to bring the new data back in.

In Paint mode, this method can also be used to define the regions of each paint layer of a landscape.

For very large landscapes however, it is recommended to do streaming level world composition [as described here](https://docs.unrealengine.com/latest/INT/Engine/LevelStreaming/WorldBrowser/)

#### Painting

We can just drop in a single material, but we can also use Paint mode to blend multiple materials on the landscape. 

- switch to the landscape tool's paint mode
- scroll down to the tool's target layers
- for each material layer, click on the + button to create layer instances (choose the "Weighted Layer" option, and save these in a sensible location)
- Now you can paint the materials
- After painting, click Build in the big toolbar to rebuild the lighting for the level

Again, there are many painting tool options to explore. 

Once built, a landscape material instance can be customized by double-clicking on it to open the editor. There are many parameters to test, and textures that can be replaced --- but it's recommended to do this *after* the landscape is created.

#### Adding water

Place a BSP box into the world so that it lies above the lowest land. Drag on the "M_Water_Lake" material from the StarterContent/Materials folder in the content browser.

It might seem weird that we can walk on water. [To make it swimmable, look here](https://wiki.unrealengine.com/Swimmable_Water_Volume_Tutorial) -- or [this video tutorial](https://www.youtube.com/watch?v=LtyXjSb1P-4). 

Very likely you want to also place a post-processing volume under the water, so that the rendering has a watery style when swimming.

#### Caves

It isn't possible for a single landscape to overhang itself, but there are other ways of making caves.

It is possible to make a hole in a landscape, so that they can be walked through, by sculpting with the visibility tool. You will first need a Hole Material, [as described here](https://docs.unrealengine.com/latest/INT/Engine/Landscape/Materials/index.html#landscapeholematerials), which you can apply as the Hole Material in the landscape's details panel. 

- Open the M_Landscape_Master material
- In the Material panel, set "Blend Mode" from "Opaque" to "Masked"
- In the graph editor, drag a cord from the Opacity Mask node and release the mouse
- Search for "vis" and select "Landscape Visibility Mask"
- Apply, save, and close the material editor
- Drag the MI_Landscape_Inst onto the "Landscape Hole Material" option of the lanscape's panel

Then make a hole in the landscape using the visibility mask, [as described here](https://docs.unrealengine.com/latest/INT/Engine/Landscape/Editing/SculptMode/index.html#visibility). 
- In Sculpt mode, select the Visibility Tool, and draw the hole(s) as needed
- You will need to rebuild lighting again.

You probably want to surround the hole with some rocks or other meshes to mask it. You probably also need to edit the material instance of the landscape to enable two-sided lighting, so that it doesn't become transparent underneath.

To fill out the cave underneath, either use BSPs & meshes with collisions, or you can also create another landscape underneath the first (but you will still probably need BSPs & meshes for the roof.)

---

### "Foliage"

The foliage tool is really just a way to randomly disperse objects over a landscape -- typically used for trees, bushes, grassy areas etc., but in fact it can be used for any kinds of objects.

You'll need some ground to start with, whether a giant BSP, mesh, or a landscape, to place objects on. Then switch to the Foliage Mode tool.

In the Paint option, drag one or more objects (meshes) into the Foliage Type box. Be very careful not to bring in very complex meshes or other complex (heavy) items that when duplicated many times could severely affect frame rate. 

Now click and drag in the viewport to paint randomized scatterings of these meshes on your ground. If you placed too many, you can also remove them by shift-clicking. 

The paint options can be used to set the area over which meshes are added, and how densely (and the shift-click erase density). The Reapply tool can be used to change parameters of existing instances. The selection tool can be used to grab individual instances for moving and deleting. With the Lasso tool, you can drag to select many instances.

You can also vary the properties of each mesh type in the foliage tool -- select the mesh, and the details panel appear below. Common settings would be varying the scale, increasing the radius (spacing between objects), random yaw (random rotations). There are many other options. Align to normal will rotate objects on sloped surfaces, ground slope angle will not place objects on extreme slopes, etc.

If you want meshes that can't be walked through: 
- make sure they have a proper collision mesh applied first (you can check this by double-clicking on the mesh, and pressing the "Collision" toolbar option -- you shoudl see a wireframe of the collision bounds appear). 
- In the foliage panel select the mesh, and in the below Instance Settings / Collision Presets choose "BlockAllDynamic"

> Note that there is a newer, more experimental [foliage creation system described here.](https://docs.unrealengine.com/latest/INT/Engine/OpenWorldTools/ProceduralFoliage/QuickStart/index.html)

---

### Volumes

*Volumes* define a 3D area for certain purposes, often invisible (such as blocking volumes to prevent actors leaving the area, pain causing volumes to inflict damage on any actor within the area, trigger volumes that cause events when an actor enters or exits them). We'll see a few volume types relating to lighting and rendering in particular.

For example, a **box trigger** can cause events to happen when a player enters or leaves the region of the (invisible) box region. 

A neat thing is to turn an existing geometry brush into a trigger. You can select a BSP geometry brush in a level, duplicate it, then in the Details panel, the Actor section, you can choose "Convert Actor" and turn it into a "Trigger Volume". 

---

### Lighting & Visual effects

The simplest way to add lights by dragging a light in from the placement mode lights panel. 

- Point lights diffuse in all directions
- Spot lights diffuse over a cone shape
- Directional lights point in only one direction, as if from a huge distance, like the sun

Once added, lights can be moved, rotated etc. like any other actor. Whenever we add or modify a light or rendering feature, or even a new material, we will need to rebuild the lighting model (press Build in the big toolbar). Unreal will tell you that lighting needs to be rebuilt.

In the Transform panel, Lights also have a Mobility option. Lights can be "static" , "stationary" (the default), or "movable". 
- Static lights are always on, and can't move. They are "baked" into the world when built, so they are effectively free. However they do not cast shadows from any moving objects. 
- Stationary lights are better in that they can also cast shadows on moving objects. No more than four non-static lights can overlap for shadowing to work -- and bear in mind that sunlight counts as one.
- Movable lights are completely dynamic in position and properties, and the most expensive to use.

Common properties of interest:

- Intensity (brightness), colour, attenuation radius (how large an area it affects)
- Source radius/length are important for the shape of specular highlights, if used with shiny materials

Aside from point/spot/directional lights, there are several other asset types that have a strong influence on the lighting visualization. Most of these can also be created by dragging in from the placement mode panel:

- To improve the lighting quality drag in a **Lightmass Importance Volume** and resize it to cover all of our objects of interest. This is especially important for VR. 

- **Sky lights** are used to capture the light coming from content in the world at large distances, such as the skybox or distant geometry. It is useful for emulating cloudy days, for example (in which you should also reduce the directional light of the sun, or remove it altogether). 

- The **Atmospheric Fog** and **Exponential Height Fog** visual effects simulate the light scattering effects of the air. Exponential Height Fog tends to make the fog denser closer to the ground, like haze. 

- To improve realism, add **Reflection Capture** actors to any important spaces in which shiny surfaces should respect the surrounding space. Frequently you would put a reflection capture in each room, for example. Box reflection captures are good for ordinary rooms, sphere reflection captures for most other spaces. Be careful not to place them too close to any particular object, or that object will dominate the reflections. Note that these reflection capture objects are very cheap, as they are pre-baked when building the project. However they might not reflect dynamic objects.

> Although a shiny material will reflect in a mirror-like way via reflection captures, it won't be perfectly accurate. To create a better mirror, we can use [Planar Reflection](https://docs.unrealengine.com/latest/INT/Engine/Rendering/LightingAndShadows/PlanarReflections/) objects. This is very expensive though. Another method to make mirrors uses screen capture objects. There's an example of this in the Content Examples project, [see docs](https://docs.unrealengine.com/latest/INT/Resources/ContentExamples/Reflections/1_7/index.html). 

- The **Post effects volume** can be used to change the rendering style. The post visual effects are only applied when the camera is within the volume's bounds. Care needs to be taken with these however: the result experienced in VR may be quite different than how it is experienced on screen, where there is less depth and immersion. See [the docs here](https://docs.unrealengine.com/latest/INT/Engine/Rendering/PostProcessEffects/index.html#postprocesssettings) for examples of some of the effects applicable.

	- Even more radical changes can be achieved using **Post Process Materials** and embedding them within a post effects volume: [see docs](https://docs.unrealengine.com/latest/INT/Engine/Rendering/PostProcessEffects/PostProcessMaterials/)

Dynamic shadows are usually a costly feature. However, this cost can be cut in half by enabling “Single Sample Shadow from Stationary Lights” on your movable Actors. This feature makes shadow receiving on dynamic objects much cheaper at the cost of some quality. It doesn’t work for all scenarios, but is worth playing around with for your dynamic objects for potentially great performance gains.


---

### Materials

Materials are assets that can be applied to geometry and meshes, and determine how light plays off them. The starter content comes with a few examples of materials, including brick, grass, chromes, plastics, etc. 

**Applying a material to an object** can be as simple as dragging the material from the content browser onto the object in the viewport. However, some objects can carry more than one material (this is evident from their details panel) -- it could be for example a lamp that uses a glassy material for the shade and a different material for the stand. In this case you can drag materials to each of the material slots in the details panel.

**Modifying materials by instancing:** Some materials also have **parameters**, which can be modified by using **Material Instances**. You can right-click on any Material to select "Create Material Instance", and save the instance in the content browser. Material Instances can be used wherever a Material was used. The main difference is that when you double-click it opens the Material Instance editor, which lists the parameters in the material and allows us to edit them. Also each one has a checkbox, which when ticked, makes them appear in the object's details panel, allowing us to customize the instance per object. Instancing is much cheaper than creating new materials, and also allows you to change properties without needing to recompile the underlying shaders.

Instance parameters can also be modified in-game via Blueprints, see [tutorial](https://docs.unrealengine.com/latest/INT/Videos/PLZlv_N0_O1gbQjgY0nDwZNYe_N8IcYWS-/srUSDU1u0og/index.html).

**Creating new materials:** You can create new materials by selecting "New Material" in the content browser menu (or by right-clicking in the content browser), and you can duplicate existing materials like any other asset. You can edit a material by double-clicking on it, which will open the Material Editor. This editor allows you to define the material properties via a data-flow graph, [as described in the tutorial here](https://docs.unrealengine.com/latest/INT/Videos/PLZlv_N0_O1gbQjgY0nDwZNYe_N8IcYWS-/lngF4VVNER4/index.html). Materials in Unreal are mainly defined in terms of:
- Base color (the underlying colour or coloured texture of the object)
- Metallic (whether shinyness is metal-like or plastic-like, usually just the value 0 or 1)
- Roughness (a roughness of 0 is like a perfect mirror, a roughness of 1 is not reflective at all)
- Specular (how bright or prominent reflective shines are)
- Emissive colour (usually zero/black, but can be increased if this material emits light, like an LED)
- Normal (emulates small deviations in the surface, such as grout between bricks; usually requires specific normal map textures)

---

### Particles

Particle systems are used to create a wide variety of effects, such as smoke, fire, fireworks, electric sparks, dust, explosions, etc. There are quite a few particle system example assets in the starter content and other free projects (especially "Particle Effects" project!)

Your world can have a number of **Emitter** actors, each of which refer to a **Particle System** asset (specified by the emitter's "Template" property in the details panel). Particle system *components* can also be added to Blueprint objects. Particle systems themselves can be edited through a built-in editor called "Cascade".

See the [tutorials](https://docs.unrealengine.com/latest/INT/Videos/PLZlv_N0_O1gbQjgY0nDwZNYe_N8IcYWS-/srUSDU1u0og/index.html).

---

### Blueprints

"Blueprint" is the visual scripting system in Unreal, which means you can do programming by connecting up visual flow charts rather than writing code. (You can also write C++ code if you want...)

Here's a gentle introduction:

<iframe width="640" height="360" src="https://www.youtube.com/embed/8WeE4q6Ba40?list=PLZlv_N0_O1gaCL2XjKluO7N2Pmmw9pvhE" frameborder="0" allowfullscreen></iframe>

Generally the form of blueprints is that **there are events that can trigger functions, and there are references to objects** that these functions can operate over. 

To add a new node to a blueprint you right-click in the background of the blueprint window, or you drag a wire off one of the pins in an existing node, to pull up the menu of nodes. The nodes that you can choose may depend on what objects you have selected in the editor. Wire colors tell you what kind of data is going down the wire. Red is booleans, green is floats, blue are objects, etc. 

The most important are white wires, which are execution wires -- things that actually make stuff happen. A special thing about white wires is that they can only have one destination. If you want to trigger two things from one event, you have to connect the Exec output of the event to Exec input of the first function, and then take the Exec output of that to the Exec input of the second function, and so forth. 

The most common places that you will do blueprints are: 

1. The **Level blueprint**, where you can write behaviours that are global to a level. Typically this is where you would put mouse/keyboard etc. interactions, for example. You can open the level blueprint from the Blueprints large tool item above the viewport.

2. **Class blueprints** are ways that you can add scripted behaviour to objects that can be spawned during a game, or which you can make multiple instances of while editing. In this case the script is contained within each instance. Class blueprints therefore also have a viewport, and can have other components such as meshes embedded.    

	- They also have a Construction Script. Within a class blueprint, the **Construction Script** is for events that fire whenever your object is created (or transformed, etc.) within the editor.  It can be used to configure the properties of a blueprint, such as enabling the visibility of a light source, or even generating procedural objects, which can be configured via variables.  [A couple of simple examples here](https://www.youtube.com/watch?v=6RqDo3012YA)  
	- You can open class blueprints from in the Content Browser, or by following links in the World Outliner. You can create new Class Blueprints from the Content Browser.  The benefit of using class BPs is that you can create many many instances of them throughout your level.

#### Variables

- We can add **variables** to the blueprint using the +variable toolbar item (for example), and set the type and name of the variable. Once created, variables can be dragged into the editor (as a getter or setter reference), and used to store state within the script. Use a variable to store some data, of whatever data. Or, to send some data to another graph within the same blueprint or another blueprint. Or, to make something configurable on a class blueprint with instances. etc.

- Note that Set variable nodes must also have an Exec input to take effect.

- Many input and output pins can be turned into a variable -- right click the pin and select "Promote To Variable" -- set the name, compile, set the default, done!

- Any variable can be made **editable** -- just tick the editable option in the details window, or, click the eyeball icon next to the variable name. This will now be editable in the details of any instance in the world. 

> Vector variables can also be 3D editable, which means they can be manipulated right in the main scene editor!

- Object type variables can be used to store references to other actors in the game. This is what you need to refer to an object that already exists in your scene.

- Class type variables are only for when you need to remember the **type** of an object. It is needed if you want to spawn new objects that don't yet exist in the level. 

#### Tips

- The blueprint editor is highly context sensitive. Whatever item you have selected in the viewport or content browser, relevant blueprint actions will come top in the blueprint context menu.

- Any object that will be moved in-game must have the "movable" option set in the transform Detail (rather than "static"). **Weirdly, the same is also true for lights. So for example, if you want to turn a light on and off dynamically, you also need to set the "movable" option.**. Yeah.
	
#### Adding comments to remind you what parts of a blueprint do

- Right-click any node and edit the text in the Node Comments section, or
- Select a few nodes, Right-click and choose Create Comment from Selection, or
- Press "C" to create a new comment box

#### Debugging

You can right-click and add a breakpoint on any event, and this will call up the blueprint editor when the event triggers in-game. At that point you can step through node by node to see exactly what is happening.

You can also add "Log" (or "Print") nodes in a blueprint, which will write text onto the viewport during the game.


---

### Embedding sound

[See documentation](https://docs.unrealengine.com/latest/INT/Engine/Audio/index.html)

You can import audio files into the Content Browser like any other content -- they become **Sound Wave** assets. 

> A handy place to find free audio files is [freesound.org](http://freesound.org) (but check licensing first!). 

Audio files should be in the WAV format (you'll have to use other software to convert files to WAV if they are e.g. MP3 format). Just press the "Import" button in the content browser, or drag them from a windows explorer into the content browser. 

The simplest way to then add the Sound Wave to your world is to drag it from the content browser to the viewport. This will create an **AmbientSound** Actor, indicated by a loudspeaker icon in the world. 

However it will normally just play once at the level start. To make it play continuously, double-clock the SoundWave in the content browser and set the **Looping** property to true. (Watch out though, to make a continuous ambience you may need to edit the soundfile to have good loop points using an external audio editor such as [Audacity](http://www.audacityteam.org).) 

The AmbientSound actor has several properties in the details panel that can be useful to modify, such as the Sound to be played, the Volume multiplier (a way to change loudness), and Spatialization.

#### Spatialization (Attenuation)

Spatialization makes sounds seem to emanate from the correct directions as you move around the world. Attenuation specifies how a sound gets more subdued as you move away from the source.

By default, most AmbientSound actors have spatialization enabled (see the Attenuation properties in the actor's details panel). However the default attenuation settings do not have much effect. You can either:
- Override the settings on a specific AmbientSound actor (only recommended for special cases), or
- Assign a **[Sound Attentuation](https://docs.unrealengine.com/latest/INT/Engine/Audio/DistanceModelAttenuation/index.html)** asset. Click on the Attenuation Settings option in the actor's details panel, and either pick an existing one, or press "Create New Asset: Sound Attenuation" in the menu. The asset is created in the content browser and you can then double-click to assign settings. The benefit of creating an asset this way is that you can re-use it many times in a world.

In either case there are many options. The most important:

- Radius (min radius): Within this distance (in cm) the sound plays at full volume.
- Falloff Distane (max radius): at this distance (in cm) the sound is no longer heard. 
- Distance Algorithm: determines how quickly the sound becomes quiet as you move away. 	
	- Use "Linear" for general looping ambience and low-detail background sounds, and large radius background sounds. Use "Logarithmic" when accuracy is important -- they'll only draw attention when very near. Or use "NaturalSound" as a softer alternative for this. Use "LogReverse" for sounds that should seem loud anywhere near. 
- The attenuation shape determines the shape of the space the sound fills. It is displayed as an orange frame around the sound actor. Generally use Sphere, but other shapes may be useful for large objects.

#### Triggering

Within a level blueprint, we can drag an AmbientSound actor in to get a reference, and then trigger from events (such as OnActorOverlap of box triggers) to call the "Play" function referencing this sound. It's also possible to spawn sound events (search for "Spawn Sound" in the blueprint editor), which can be useful for short sound effects associated with other actions in the world.

For more interesting triggered sounds we'd want to use a Class Blueprint.

- Let's make a simple cylinder in the world. Drag in Cylinder from the Basic section of the Placement Mode panel. 
- Now in the details panel, select the blue button Blueprint/Add, and choose where in the content browser to save it, and give it a more useful name.
- In the blueprint editor's viewport, click +Add Component and choose Box Collision. Scale this up so that it is larger than the cylinder. 
- In the details panel, scroll down to Events, and press the + button for "On Component Begin Overlap". This opens the blueprint editor.
- Again press +Add Component, but this time select Audio. This is like an AmbientSound actor, but within a Class blueprint it is called Audio Component. In the details panel, choose a Sound to play, Attenuation settings, etc. as normal. Turn the "Activation: Auto Activate" property off.
- Now drag the audio component into the blueprint editor to create a reference. Drag a line from the outpin, and under Audio/Components/Audio choose "Play". Drag a white line from the OnComponentBeginOverlap white outpin to the white pin of "Play".
- Now save the blueprint & go back to the main level editor. 

Now you can make many copies of this object in the world, and edit them all in one place.

To take this further, let's make it possible to change the sound associated with each instance. 
- In the class blueprint editor open the Construction script tab.  
- Drag in a reference to the Audio component, and drag a line of it. Choose the Audio/Component/Audio/Set Sound function. 
- Connect the White exec pin of the Construction Script node to the Set Sound node. 
- On the Set Sound node's "New Sound" pin, right click and choose "promote to variable". This makes a new variable node appear. 
- In the details panel, change the name of this node to something sensible (e.g. "sound to play"), and tick the Editable box. Also, in the Default Value option, set a sound to play.
- Compile & save the blueprint, then return to the level. You should now be able to create copies of the actor, and in the details panel, see & change the "sound to play" variable of each one independently.

#### More complex sounds

The AmbientSound actor also has a "Modulation" property set, which can be used to set ranges of pitch (rate) and volume (loudness) over which triggered sounds may randomly vary. This can enrich the soundscape relatively easily.

More complex sound sources than simple SoundWaves can be created as [Sound Cues](https://docs.unrealengine.com/latest/INT/Engine/Audio/SoundCues/Editor/index.html), which have their own editor similar to the material editor or blueprint editor, and can have additional instance properties. With this editor multiple sounds can be mixed together, with different modulation and other effects. Sound Cues can be used anywhere instead of a Sound Wave (i.e. in AmbientSound actors and class blueprint Audio components.

You can also create an "Audio Volume" (from the "Volumes" option of the placing mode tool, see [here](https://docs.unrealengine.com/latest/INT/Engine/Actors/Volumes/index.html)), a region of space with specific audio properties such as how a space creates reverberation, rather like how the Post Effects volume can change visual properties.  See the details panel under Reverb, and choose or create a Reverb Effect. Reverb effects have properties like the echo density, overall reverb gain, air absorption, and more, to craft its unique sonic character. Also, Audio Volumes can be useful to define the effect of sounds being inside or outside enclosed spaces, using the Ambient Zone settings. 

---

### Converting: Brush to Mesh, Actors to Blueprint, ...

**Brush to Mesh**: Although it can be convenient to mark up a level using BSPs ("Geometry Brushes"), it is not recommended to keep them in the final project, as they can slow down performance, reduce the quality of lighting, etc. You might also want to convert a brush to mesh so that you can use it in blueprints, for foliage, and other places where BSPs cannot be used.

You can convert any BSP into a mesh in the Details Panel / Brush Settings / Create Static Mesh. Note however that after conversion, the geometry will no longer be editable. 

You can also convert multiple BSPs into a single mesh by selecting them all, right-clicking, and choosing Convert Actors to Static Mesh

You might also need to then edit the new mesh to create a collision profile for it (see physics below).

**Actors to Blueprint**: You create new blueprints from an object in the scene by hitting the blueprint button at the top of the details panel. But you can also convert **multiple** actors into a single blueprint:
- Select the objects in the viewport
- In the main toolbar above the viewport, under Blueprints / Convert Selected Components to Blueprint Class..." 

Note that you also need to convert brushes to meshes first if you want to use them in a blueprint!

**Merge multiple actors to single actor**: Reducing the number of actors can improve performance in large projects. You can merge mesh actors together right in the level viewport. This allows you to reduce draw calls to get better performance. [See docs](https://docs.unrealengine.com/latest/INT/Engine/Actors/Merging/)
- Select actors to merge
- Window / Developer Tools / Merge Actors

**Convert actor to static mesh:** Sometimes you might have a complex object or actor that you want to simply grab as a plain mesh. On the actor in the viewport right-click and choose "Convert to Static Mesh". As usual it asks where to place this new mesh asset in the content browser. An example of where you might want to do this is in creating a manikin mesh from a character blueprint.

### Beyond static meshes

#### Instanced meshes

Using Instanced Static Meshes (ISMs) or Hierarchical Instanced Static Meshes (HISMs) allows vastly larger numbers of objects in a scene with little performance penalty. This is a bit like how the Foliage tool works. The limitations are that instances should all be the same mesh & material, with only transform (position, rotation, scale) being different. There's a way to use blueprints to create them. The HISM variant adds level of detail (LOD) mesh variants, which can also make a massive performance gain by reducing mesh detail for more distant objects. Of course, you need to create the lower poly meshes first!

[See this video here](https://www.youtube.com/watch?v=oMIbV2rQO4k)

#### Procedural meshes

You can add Procedural Mesh Component to an Actor and provide Vertices, Triangles and UVs to generate a mesh that is rendered at the actors location. Or you can use the Copy from Static Mesh function to initialize it, then start modulating it. Some methods of interest:
- ```Copy Procedural Mesh from Static Mesh Component```: with this function you can create a procedural mesh from any existing mesh. 
- ```Slice Procedural Mesh```: with this players can cut a mesh into pieces, as if cutting cheese. 

[Docs](https://docs.unrealengine.com/latest/INT/BlueprintAPI/Components/ProceduralMesh/index.html)

---

### Physics

[See main docs](https://docs.unrealengine.com/latest/INT/Engine/Physics/index.html)

Physics simulations can add a lot of immersive *agency* to a world, especially when coupled with VR controllers allowing you to pick things up, swing things, throw things, balance things, and so on. 

A static mesh actor can have physics enabled by ticking the box in the Details panel. If this button is greyed out, it might be that you need to change the mobility option to Moveable (just under the position/rotation/scale transform). Or it may be greyed out if the object doesn't have a collision profile defined. To check whether a mesh has a collision profile, edit the mesh and enable the Collision button in the toolbar. If no purple collision lines show up, you can add a new collision hull like this:
- double-click the mesh to open the mesh editor, 
- to generate the hull automatically:
	- from the Collision menu choose a collision option
		- For regular shapes, a sphere/capsule/box collision might be good enough
		- For most other cases, use Auto Convex Collision
			- in the details panel that opens, set the accuracy options, and click Apply
	- a visualization of the collision hull will appear in the viewport
- otherwise, with other options in this menu you can build a hull by hand
- finally, save and close the mesh editor	
[See here for more on mesh collision options](https://docs.unrealengine.com/latest/INT/Engine/Content/Types/StaticMeshes/HowTo/SettingCollision/)

Gravity can be enabled or turned off on each object's details panel, and the mass of objects (in kg) can be set. Lock position/lock rotation options limit a physics object's movements to planes/lines/points, and rotational axes. The Collision Complexity option can be set to Complex for more accurate shape collisions (rather than using the simplified collision profiles). 

Also damping can be applied (simulating drag, to limit accelerations). There are both Linear Dampling and Angular Damping. For reference, a Linear Damping value of 30 is enough to counteract gravity.  

You can also apply different Physical Materials to physics objects; Physical Materials can define friction, restitution (i.e. bounciness), among [other properties](https://docs.unrealengine.com/latest/INT/Engine/Physics/PhysicalMaterials/Reference/index.html).

#### Collision types and events

Collisions can generate three kinds of events:
- Ignore: objects pass through each other, and no events are created
- Overlap: objects can pass through each other, but generate ReceiveBeginOverlap and ReceiveEndOverlap events (like with the box trigger, for example), if both actors' Generate Overlap Events options are ticked.
- Block: objects cannot pass through each other, and may generate ReceiveHit or OnComponentHit  events if the  Simulation Generates Hit Events option is ticked. 

There are many types of objects, each can have different Ignore/Overlap/Block options with other types. See the Details panel Collision Preset, and expand it to see what each preset implies. [More on collision](https://docs.unrealengine.com/latest/INT/Engine/Physics/Collision/Overview/index.html)

#### Constraints

Physics Constraint Actors can be used to limit to motion of physics objects in specific ways. You can find them in the All Classes section of Placement Mode. 

Constraint *Components* can also be added to a Blueprint object, in a similar way.  

In the details pane, select two actors for Constraint Actor 1 and 2. At least one of these actors must have physics enabled.
Then choose what kinds of constraints you want to apply (see [here](https://docs.unrealengine.com/latest/INT/Engine/Physics/Constraints/ConstraintsReference/index.html) for a full list). This can limit motion along a plane or line, limit rotation to certain angles, etc. It can also be used to apply a motor to drive motion or rotation to the constraint.

A good place to check out the options is in the [Content Examples](https://docs.unrealengine.com/latest/INT/Resources/ContentExamples/Physics/index.html) project's Physics map.

Constraints can also have [damping settings](https://docs.unrealengine.com/latest/INT/Engine/Physics/Constraints/DampingAndFriction/index.html).

#### Destructibles

- Right-click on any Static Mesh asset in the content browser and select 'Create Destructible Mesh'. 
- Open up the new destructible mesh by double-clicking.
- Choose the number of cells to divide into (under Voronoi in the Details panel)
- Also in details, select Enable Impact Damage (so that physics collisions will make it break)
- Set the Impact Damage (e.g. 1.0) needed to make it break
- Set Impact Damage Depth to 0  
- press Fracture Mesh in the tool bar to see how it looks
- Drag it into the level somewhere above ground, and enable physics on it. 

[For more advanced destructible behaviour, look at the Content Examples' Destructible map](https://docs.unrealengine.com/latest/INT/Resources/ContentExamples/Destructables/index.html)

#### Traces (Raycasts)

Sometimes you want to know what the user (or another actor) is looking at, or whether something can be seen. This is easily accessible from within blueprints, as [described here](https://docs.unrealengine.com/latest/INT/Engine/Physics/Tracing/index.html).

---


### Basic "AI"

The model of artificial intelligence for agents (or Non-Player Characters, NPCs) provided by Unreal comprises a loop of Sensing, Decision-making, and Acting:

- **Sensing**: represented in Unreal as a "Blackboard", this is simply a set of variables that are updated often at fairly low frequencies, and remain available for the agent's decision making processing. It's a bit like short-term memory, in both echoic and working memory forms. Sensing may involve things like navigation mesh queries, physics queries, detecting nearby objects/agents, etc. and thus can sometimes be quite expensive. It can also represent internal states or persistent knowledge of the agent, such as whether it knows somebody, whether it is hungry, etc. Blackboards could be updated via Services, AI Components, or general Blueprints. Basically, the blackboard needs to contain all the data, or facts, upon which decisions are made.

- **Decision-making**: represented in Unreal via *Behaviour Trees*. These are a kind of flow-chart of tasks driven mainly by conditional logic on task success or failure. Nodes are either tasks, or composites that can trigger multiple tasks, and can have a number of conditions determining whether they can be executed. They're very cheap, but design-intensive. Some decisions will react/respond to external stimuli; others respond to internal goals etc. But generally, behaviour trees are designed around a kind of prioritized list of condition-action pairs.

- **Action execution**: represented in Unreal via *Tasks*, making the agent do something in the world. Tasks often take some time to complete, and can be usually defined as either running or completed (successfully or not). Generally when a task completes, the success status returns to the behaviour tree to figure out what task to start next.

Building an AI agent requires a few components:

- An **AI Character** blueprint, which is the container you see in the world; the "body". It may also contain animation blueprints etc. for animating the character model. 

- An **AI Controller** blueprint, the "head", which stores the blueprints for behaviour, including the **Behaviour Tree**, which forms the "brain", including the **Blackboard**, which stands in for memory, and any **Tasks** involved. 
	
One way of thinking it is that a Character asset could more or less be controlled either by a player or by an AI Controller; in that regard the AI Controller is a bit like a brain behind a virtual joystick. Breaking up the AI into all these parts makes them interchangeable. So, lots of agents can be made from the same character, lots of agents can use the same AI controller, different controllers can use the same Tasks, etc. But, it also means there's quite a few steps to wire up just to get started:

- Create a Blackboard first, and then a Behaviour Tree, by right-clicking in the Content Browser and selecting each of these types in turn. Remember to name these assets well!
	- Open up the Behaviour Tree to make sure it is using the correct Blackboard you just made.
- Create a Controller by making a new blueprint in the content browser, and making sure to set the parent class to "AIController". Now we need to tell this controller to start using our behaviour tree when the level begins:
	- Open up the Controller's blueprint event graph
	- create an Event Begin Play node, 
	- drag a cable out to create a Run Behaviour Tree node, 
	- set it to use your new Behaviour Tree.
- Now you just need a character. A quick way to make a new character is to duplicate and modify the player character, then:
	- open up event graph and delete everything, and also delete the camera/boom components
	- under details, Pawn, AI Controller class, set to the controller blueprint you just made.

At this point, the jobs to do include:
- adding some 'keys' to the blackboard. A simple example would be a location, to where the actor should walk.
- adding blueprints to the controller event graph to populate these blackboard keys using various Set Blackboard functions in the event graph. There should be at least including one driven by Event Begin Play, but of course these can be updated by other events. For our example above, it could be by getting the location of a *Target Point* actor in the world, or the location of the player, etc.
- adding nodes to the behaviour tree to determine actions to perform according to blackboard values
- adding Services to the behaviour tree that update blackboard values
- adding Tasks to the behaviour tree that define new actions

> Tip: if you are using any Move To nodes/functions in the AI behaviour, make sure to create a Navigation Volume in the level that covers your area. You can use the P key in the viewport to toggle whether the navigable area is shown or not.

However... [are Behaviour Trees a thing of the past?](http://www.gamasutra.com/blogs/JakobRasmussen/20160427/271188/Are_Behavior_Trees_a_Thing_of_the_Past.php)

---

### VR notes

Unreal's VR template offers support for both gamepad (e.g. Xbox controller) and motion controllers (e.g. Oculus Touch or HTC Vive wands). The template comes with a number of rendering optimizations for VR. [The template is described in detail here, including how to navigate it](http://www.tomlooman.com/vrtemplate/).

For teleport to work, your world requires a Nav Mesh Bounds Volume. If you don't already have one of these in your world, drag one in from placement mode's Volumes, and scale it up to your world. You can verify what locations are viable to teleport to by clicking in the viewport and pressing the P key (it will toggle the nav mesh visualization -- the teleportable areas will be highlighted green.)

To turn a Mesh into something that can be picked up and thrown around, it needs to be a blueprint. The easiest way is like this:
- Create a new Blueprint (right click in Content Browser, New Blueprint)
- under All Classes, type "static" and select StaticMeshActor; save in a sensible location.
- Now in the blueprint editor click Class Settings to show the class settings details pane. Under Interfaces, click Add, and choose PickupActorInterface from the drop-down.
- Now copy the entire Event Graph from BP_Pickup_Cube into your new blueprint.
- Now in the Static Mesh Component details, choose which mesh to use. Make sure that your mesh has a collision profile applied to it.
- Also in the details, enable Physics/Simulate Physics, Collision/Simulation Generates Hit Events, and Collision/Generate Overlap Events


#### Getting an existing world into a VR project

Because of various factors (including specific rendering settings) it makes more sense to create a new project based on the VR template, and migrate your existing world's content into it. So long as you are careful in how you organize your content, it should be possible. It's often a good idea therefore to keep the number of files/folders at the top-level of your Content to a minimum. Bear in mind that not all of a project can be migrated; things like project settings need to be manually duplicated if needed. 

> For example, it *is* possible to migrate player pawns between VR and FPS template projects, in the same way as migrating other assets. However they won't work until the Input mappings are copied too, which has to be done manually in the project settings. Moreover, for the First Person Character, you may also need to set the Default Game Mode to First Person Game Mode (either in Project Settings or your level's World Settings). Then you can drop "HMD Locomotion Pawn" and "First Person Character" into the same world, just make sure that only one of them has the "Auto Possess Player" set to "Player 0" in the details pane. 

---

### Using C++

[Main documentation entry point is here](https://docs.unrealengine.com/latest/INT/Programming/index.html)

To be able to use C++ features, on Windows, Visual Studio 2015 needs to be installed; on OSX, Xcode needs to be installed. Both have free versions, both are big downloads. Generally, the idea is to use C++ to build classes that can then be used by designers in ordinary Blueprints. Typically you want to add C++ features either to improve performance in critical parts of a game, or to add features that are not provided by Unreal already.

If you want/need C++ in a project, and your project was a Blueprints-only project (i.e. you haven't added any C++ classes yet), you can convert it to a C++ project by simply adding a new C++ class. 

Add a new C++ class to a UE4 project by going to File > New C++ Class. Pick a base class (e.g. Actor) and it will create all the files we need, including a Visual Studio solution, and a Source folder, in your project; and it will open Visual Studio automatically. After editing code, compile either by Build in Visual Studio, or Compile in Unreal Editor. 

The actor show now show up in the Content Browser under the C++ Classes section. We can drag it into the world, add components, etc. 

[We can now also create new Blueprints that derive from our C++ class](https://docs.unrealengine.com/latest/INT/Gameplay/ClassCreation/CodeAndBlueprints/index.html) (create new blueprint as normal, set the C++ class as the base class for the blueprint). Or, we can create blueprints that modify specific instances of a C++ actor (With a C++ actor instance selected in the world,  click the Blueprint/Add Script button in the Details Panel.) While editing the new blueprint's graph, C++ implemented events and functions become available for use.


#### Adding a property

```
	UPROPERTY(EditAnywhere) int32 TotalDamage;
	UPROPERTY(EditAnywhere, BlueprintReadWrite, Category="Damage") int32 TotalDamage;
```

Remember to also initialize the property in the class constructor.

> [See here for all the property options](https://docs.unrealengine.com/latest/INT/Programming/UnrealArchitecture/Reference/Properties/Specifiers/index.html)

#### Adding functions/events

Expose a function to be callable via blueprints:

```
	UFUNCTION(BlueprintCallable) void CalculateValues();
```

Expose an event to blueprints (i.e. a way to call back from C++ into a blueprint):

```
	UFUNCTION(BlueprintImplementableEvent, Category="Damage") void CalledFromCpp();
```

Expose a function to be both callable (with a default implementation) but also overrideable (replacing implementation) in blueprints:å

```
	// if you want to also provide a default C++ implementation, where user did not provide one via blueprint:
	UFUNCTION(BlueprintNativeEvent, Category="Damage") void CalledFromCpp();
	void CalledFromCpp_Implementation() { ... }
```

To just bind static functions to blueprints, rather than objects & methods, see the [blueprint function library docs](https://docs.unrealengine.com/latest/INT/Programming/BlueprintFunctionLibraries/index.html)

#### Base classes / concepts

**UObject**: most basic class provides reflection, serialization, & networking of properties and methods, etc. Marked by UCLASS(). 

**UStruct**: simpler than UObject, it does not provide garbage collection etc., but does allow plain-old-data structs to be exposed to blueprints, serialization, networking etc. Marked by USTRUCT(). 

**AActor**: a UObject intended to be placed or [spawned](https://docs.unrealengine.com/latest/INT/Programming/UnrealArchitecture/Actors/Spawning/index.html) in a level, and which contains Components. The RootComponent holds a UActorComponent or USceneComponent which can contain many other UActorComponents. Actors themselves do not have transforms, and thus do not have locations, rotations, or scales. Instead, they rely on the transforms of their Components; more specifically, their RootComponent. Actors have events such as BeginPlay, Tick, EndPlay. [See docs](https://docs.unrealengine.com/latest/INT/Programming/UnrealArchitecture/Actors/index.html). Actors can be removed by calling Destroy().

**UActorComponent**: are attached to Actors on creation. USceneComponent is a specialization that has a spatial transform relative to its parent. UPrimitiveComponents are USceneComponents that also have a visible representation. etc. All of these components can have their own TickComponent() events, etc. [See this tutorial for how to add components to an actor via code](https://docs.unrealengine.com/latest/INT/Programming/Tutorials/Components/index.html)

#### Binding 3rd party libraries

- When building libraries:
	- Build for Multi-threaded DLL (/MD) with 32- and 64-bit versions
- In the Unreal project:
	- add folder /ThirdParty/libraryname and inside add /include and /lib

[Unreal has its own build system, documented here](https://docs.unrealengine.com/latest/INT/Programming/UnrealBuildSystem/index.html). To configure to add a library to the build system, [there are some suggestions here](https://answers.unrealengine.com/questions/396982/cant-build-project-in-ue-412.html).

Beyond simple cases, it might be worth packaging the library binding as a [plugin](https://docs.unrealengine.com/latest/INT/Programming/Plugins/index.html), so that it can be re-used in many projects.

[Example: binding OpenCV to Unreal](https://wiki.unrealengine.com/Integrating_OpenCV_Into_Unreal_Engine_4). [Another example](https://github.com/shadowmint/ue4-static-plugin/).

> Possibly useful: UE4 gives callbacks right after dll load and before unload. 


</script>
</div>
</div>
</div>
<script src="js/marked.js"></script>
<script>

var renderer = new marked.Renderer();
var toc = []; // your table of contents as a list.
renderer.heading = function(text, level) {
	var slug = text.toLowerCase().replace(/[^\w]+/g, '-');
	if (level == 2) {
		toc.push("- [" + text + "](#"+slug+")");
	}
	return "<h" + level + " id=\"" + slug + "\"><a href=\"#" + slug + "\" class=\"anchor\"></a>" + text + "</h" + level + ">";
};

var convertMarkdown = function(text) {
	toc = [];
	var body = marked(text, { renderer: renderer });
	document.getElementById('toc').innerHTML = marked(toc.join("\n"));
	return body;
};

var body = document.getElementById('sourcetext').innerText;
document.getElementById('main_body').innerHTML = convertMarkdown(body);

function addyoutube(element, id) {
	// Load the image asynchronously
    var image = new Image();
	image.src = "https://img.youtube.com/vi/"+ id +"/0.jpg";
	image.addEventListener( "load", function() { element.appendChild(image); }(i));
	
   	// click to play
    element.addEventListener( "click", function() {
    	this.innerHTML = '<iframe width="640" height="360" src="https://www.youtube.com/embed/'+id+'?rel=0" frameborder="0" allowfullscreen></iframe>';
    });
}

var youtube = document.querySelectorAll( ".youtube" );
for (var i = 0; i < youtube.length; i++) addyoutube(youtube[i], youtube[i].dataset.embed);

</script>
</body>
</html>